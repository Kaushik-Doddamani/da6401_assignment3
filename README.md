# Dakshina Transliteration Seq2Seq

This repository contains solutions for character-level transliteration of Dakshina languages (Hindi) using:

* A **vanilla** RNN/LSTM/GRU Seq2Seq model (`solution_1.py`, `solution_2.py`, `solution_4.py`)
* An **attention-augmented** Seq2Seq model (`solution_5_model.py`, `solution_5.py`, `solution_5b.py`)
* An **interactive connectivity** visualization of attention weights (`solution_6.py`)
* Jupyter notebooks demonstrating training and evaluation:

  * `notebooks/solution_vanilla.ipynb`
  * `notebooks/solution_attention.ipynb`

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ checkpoints/                # Saved model checkpoints
â”‚   â”œâ”€â”€ best_seq2seq.pt
â”‚   â””â”€â”€ best_attention.pt
â”œâ”€â”€ configs/                    # W&B sweep configurations
â”‚   â”œâ”€â”€ sweep_config.yaml
â”‚   â””â”€â”€ sweep_attention.yaml
â”œâ”€â”€ fonts/                      # Devanagari fonts for plotting
â”œâ”€â”€ lexicons/                   # Dakshina train/dev/test TSV files
â”œâ”€â”€ notebooks/                  # Demo notebooks
â”‚   â”œâ”€â”€ solution_vanilla.ipynb
â”‚   â””â”€â”€ solution_attention.ipynb
â”œâ”€â”€ predictions_vanilla/        # Vanilla model outputs (TSV/CSV + sample PNG)
â”œâ”€â”€ predictions_attention/      # Attention model outputs (TSV/CSV + heatmaps)
â”œâ”€â”€ solution_1.py               # Q1: Vanilla Seq2Seq implementation
â”œâ”€â”€ solution_2.py               # Q2: W&B sweep driver for vanilla model
â”œâ”€â”€ solution_4.py               # Q4: Evaluate vanilla model on test set
â”œâ”€â”€ solution_5_model.py         # Q5: Attention Seq2Seq model definition
â”œâ”€â”€ solution_5.py               # Q5: W&B sweep driver for attention model
â”œâ”€â”€ solution_5b.py              # Q5.b: Evaluate attention model on test set
â”œâ”€â”€ solution_6.py               # Q6: Interactive attention heatmap generator
â”œâ”€â”€ connectivity.html           # Exported connectivity visualization
â”œâ”€â”€ part_a.ipynb                # Starter code / reference notebook
â””â”€â”€ README.md                   # This file
```

---

## âš™ï¸ Installation

```bash
git clone <repo-url>
cd <repo-directory>
# (Optional) create a virtual environment
pip install -r requirements.txt
```

**Key dependencies:** PyTorch, pandas, numpy, wandb, plotly, matplotlib, wcwidth.

---

## ğŸ“¦ Data

Place the Dakshina Hindi lexicon files under `lexicons/`:

* `hi.translit.sampled.train.tsv`
* `hi.translit.sampled.dev.tsv`
* `hi.translit.sampled.test.tsv`

Format: `native_word \t romanized_word \t count`.

---

## ğŸš€ Usage

### 1. Train & evaluate vanilla Seq2Seq (Q1â€“Q4)

```bash
# 1a) Single training run
python solution_1.py --train_tsv lexicons/hi.translit.sampled.train.tsv \
                    --dev_tsv   lexicons/hi.translit.sampled.dev.tsv \
                    --test_tsv  lexicons/hi.translit.sampled.test.tsv \
                    --embedding_size 256 --hidden_size 512 --cell LSTM \
                    --encoder_layers 2 --decoder_layers 2 --epochs 15

# 1b) Hyperparameter sweep
python solution_2.py --mode sweep --sweep_config configs/sweep_config.yaml \
                    --wandb_project transliteration --wandb_run_tag vanilla \
                    --train_tsv lexicons/...train.tsv \
                    --dev_tsv lexicons/...dev.tsv --test_tsv lexicons/...test.tsv \
                    --sweep_count 30

# 1c) Evaluate best model on test set
python solution_4.py --train_tsv lexicons/...train.tsv \
                    --dev_tsv   lexicons/...dev.tsv \
                    --test_tsv  lexicons/...test.tsv \
                    --checkpoint checkpoints/best_seq2seq.pt \
                    --output_dir predictions_vanilla \
                    --wandb_project transliteration --wandb_run_tag eval_vanilla
```

### 2. Train & evaluate attention Seq2Seq (Q5)

```bash
# 2a) Hyperparameter sweep with attention
python solution_5.py --mode sweep --sweep_config configs/sweep_attention.yaml \
                    --wandb_project transliteration --wandb_run_tag attention \
                    --train_tsv lexicons/...train.tsv \
                    --dev_tsv lexicons/...dev.tsv --test_tsv lexicons/...test.tsv \
                    --sweep_count 30

# 2b) Evaluate best attention model
python solution_5b.py --train_tsv lexicons/...train.tsv \
                     --dev_tsv   lexicons/...dev.tsv \
                     --test_tsv  lexicons/...test.tsv \
                     --checkpoint checkpoints/best_attention.pt \
                     --output_dir predictions_attention \
                     --wandb_project transliteration --wandb_run_tag eval_attention
```

### 3. Visualize attention connectivity (Q6)

```bash
python solution_6.py --checkpoint checkpoints/best_attention.pt \
                     --train_tsv lexicons/...train.tsv \
                     --test_tsv  lexicons/...test.tsv \
                     --n_examples 5 --output_html connectivity.html \\  
                     --wandb_project transliteration --wandb_run_tag connectivity
```

---

## ğŸ““ Notebooks

* **`solution_vanilla.ipynb`**: Interactive walkthrough of Q1â€“Q4.
* **`solution_attention.ipynb`**: Interactive walkthrough of Q5 and Q6.

Feel free to run these notebooks to visualize training curves, sample predictions, and attention heatmaps inline.

---

## ğŸ“‘ License & Acknowledgments

This project is for academic purposes (DA6401 Assignment 3).
Inspired by tutorials on Keras, PyTorch, and the â€œConnectivityâ€ blog post.
