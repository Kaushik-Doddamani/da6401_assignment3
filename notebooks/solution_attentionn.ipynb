{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04328207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end training script for Q1 on the Dakshina Hindi lexicon.\n",
    "\n",
    "This script:\n",
    "  1. Loads and preprocesses the Hindi transliteration lexicon into PyTorch datasets.\n",
    "  2. Builds a flexible Seq2Seq model with configurable embedding size, hidden size,\n",
    "     cell type (RNN/LSTM/GRU), number of layers, and choice of character‐vector methods.\n",
    "  3. Trains the model with teacher forcing (optionally sampling by attestation counts).\n",
    "  4. Evaluates on dev/test sets and prints perplexities.\n",
    "  5. Runs a few qualitative transliteration examples.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "    python train_dakshina_seq2seq.py \\\n",
    "      --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "      --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv   \\\n",
    "      --test_tsv  ./lexicons/hi.translit.sampled.test.tsv  \\\n",
    "      --embedding_size 256 \\\n",
    "      --hidden_size    512 \\\n",
    "      --encoder_layers 2 \\\n",
    "      --decoder_layers 2 \\\n",
    "      --cell LSTM \\\n",
    "      --epochs 15 \\\n",
    "      --embedding_method svd_ppmi \\\n",
    "      --use_attestations\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "# Use GPU if available\n",
    "DEFAULT_DEVICE = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ─────────────────────── 1. Vocabulary helpers ───────────────────────\n",
    "SPECIAL_TOKENS = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
    "\n",
    "\n",
    "class CharVocabulary:\n",
    "    \"\"\"Character-level vocabulary that handles <pad>, <sos>, and <eos>.\"\"\"\n",
    "    def __init__(self, characters: List[str]):\n",
    "        unique_chars = sorted(set(characters))\n",
    "        # string→index and index→string maps\n",
    "        self.stoi: Dict[str, int] = {\n",
    "            **SPECIAL_TOKENS,\n",
    "            **{ch: idx + len(SPECIAL_TOKENS) for idx, ch in enumerate(unique_chars)}\n",
    "        }\n",
    "        self.itos: Dict[int, str] = {idx: ch for ch, idx in self.stoi.items()}\n",
    "\n",
    "    def encode(self, text: str, *, add_sos: bool = False, add_eos: bool = True) -> List[int]:\n",
    "        \"\"\"Convert a string to list of token ids (with optional <sos>/<eos>).\"\"\"\n",
    "        ids = [self.stoi[ch] for ch in text]\n",
    "        if add_eos:\n",
    "            ids.append(self.stoi[\"<eos>\"])\n",
    "        if add_sos:\n",
    "            ids.insert(0, self.stoi[\"<sos>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert ids back to string (stop at <eos>).\"\"\"\n",
    "        chars: List[str] = []\n",
    "        for idx in ids:\n",
    "            if idx == self.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            chars.append(self.itos.get(idx, \"\"))\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of tokens in vocabulary.\"\"\"\n",
    "        return len(self.stoi)\n",
    "\n",
    "\n",
    "# ───────────────────────── 2. Dataset class ─────────────────────────\n",
    "class DakshinaLexicon(Dataset):\n",
    "    \"\"\"Loads a Dakshina *lexicon* TSV and encodes (source, target) pairs.\n",
    "\n",
    "    TSV columns: native_word, romanized_word, count\n",
    "    We treat romanized_word as source and native_word as target.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tsv_path: str | Path,\n",
    "        source_vocab: Optional[CharVocabulary] = None,\n",
    "        target_vocab: Optional[CharVocabulary] = None,\n",
    "        *,\n",
    "        build_vocabs: bool = False,\n",
    "        use_attestations: bool = False\n",
    "    ):\n",
    "        # Read TSV – three columns, ensure correct dtypes\n",
    "        dataframe = pd.read_csv(\n",
    "            tsv_path, sep=\"\\t\", header=None,\n",
    "            names=[\"target_native\", \"source_roman\", \"count\"],\n",
    "            dtype={\"target_native\": str, \"source_roman\": str, \"count\": int}\n",
    "        ).dropna()\n",
    "\n",
    "        # Optionally keep annotator counts for sampling or weighting\n",
    "        self.example_counts: Optional[List[int]] = (\n",
    "            dataframe[\"count\"].tolist() if use_attestations else None\n",
    "        )\n",
    "\n",
    "        # Keep only the (src, tgt) pairs\n",
    "        self.word_pairs: List[Tuple[str, str]] = list(zip(\n",
    "            dataframe[\"source_roman\"], dataframe[\"target_native\"]\n",
    "        ))\n",
    "\n",
    "        # Build new or reuse provided vocabularies\n",
    "        if build_vocabs:\n",
    "            assert source_vocab is None and target_vocab is None, (\n",
    "                \"Cannot pass existing vocabs when build_vocabs=True\"\n",
    "            )\n",
    "            # collect all chars\n",
    "            all_src_chars = [ch for src, _ in self.word_pairs for ch in src]\n",
    "            all_tgt_chars = [ch for _, tgt in self.word_pairs for ch in tgt]\n",
    "            source_vocab = CharVocabulary(all_src_chars)\n",
    "            target_vocab = CharVocabulary(all_tgt_chars)\n",
    "\n",
    "        assert source_vocab is not None and target_vocab is not None, (\n",
    "            \"Must provide or build vocabularies\"\n",
    "        )\n",
    "        self.src_vocab, self.tgt_vocab = source_vocab, target_vocab\n",
    "\n",
    "        # Encode all pairs once for efficiency\n",
    "        self.encoded_pairs: List[Tuple[List[int], List[int]]] = [\n",
    "            (self.src_vocab.encode(src),\n",
    "             self.tgt_vocab.encode(tgt, add_sos=True))\n",
    "            for src, tgt in self.word_pairs\n",
    "        ]\n",
    "        # Also keep just the source sequences for SVD/PPMI embedding\n",
    "        self.encoded_sources: List[List[int]] = [src for src, _ in self.encoded_pairs]\n",
    "\n",
    "        # Padding token id\n",
    "        self.pad_id: int = self.src_vocab.stoi[\"<pad>\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded_pairs)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[List[int], List[int]]:\n",
    "        return self.encoded_pairs[index]\n",
    "\n",
    "\n",
    "def collate_batch(\n",
    "    batch: List[Tuple[List[int], List[int]]],\n",
    "    pad_id: int\n",
    ") -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Pad source and target sequences to uniform length within a batch.\"\"\"\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(s) for s in src_seqs], dtype=torch.long)\n",
    "    tgt_lengths = torch.tensor([len(t) for t in tgt_seqs], dtype=torch.long)\n",
    "\n",
    "    max_src_len = src_lengths.max().item()\n",
    "    max_tgt_len = tgt_lengths.max().item()\n",
    "\n",
    "    padded_sources = torch.full((len(batch), max_src_len), pad_id, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(batch), max_tgt_len), pad_id, dtype=torch.long)\n",
    "\n",
    "    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
    "        padded_sources[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "        padded_targets[i, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "\n",
    "    return padded_sources, src_lengths, padded_targets\n",
    "\n",
    "\n",
    "# ──────────────────── 2.5. Embedding-method modules ────────────────────\n",
    "class OneHotEmbedding(nn.Module):\n",
    "    \"\"\"Convert token ids → explicit one-hot → linear projection to embedding_dim.\"\"\"\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        # Project a one-hot vector of length vocab_size → embedding_dim\n",
    "        self.projection = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # token_ids: (B, T)\n",
    "        one_hot = F.one_hot(token_ids, num_classes=self.vocab_size).float()  # (B,T,V)\n",
    "        # zero out pad positions if desired\n",
    "        one_hot[token_ids == self.padding_idx] = 0.0\n",
    "        # project → (B, T, D)\n",
    "        return self.projection(one_hot)\n",
    "\n",
    "\n",
    "class CharCNNEmbedding(nn.Module):\n",
    "    \"\"\"Convert token ids → one-hot → conv1d over time → (B,T,embedding_dim).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        kernel_size: int = 3,\n",
    "        num_filters: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_filters = num_filters or embedding_dim\n",
    "        # Convolution: in_channels=vocab_size, out_channels=num_filters\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=vocab_size,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            bias=False\n",
    "        )\n",
    "        # Optionally project filters → embedding_dim\n",
    "        if self.num_filters != embedding_dim:\n",
    "            self.projection = nn.Linear(self.num_filters, embedding_dim, bias=False)\n",
    "        else:\n",
    "            self.projection = None\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # token_ids: (B, T)\n",
    "        one_hot = F.one_hot(token_ids, num_classes=self.vocab_size).float()  # (B,T,V)\n",
    "        x = one_hot.permute(0, 2, 1)  # (B, V, T)\n",
    "        x = self.conv1d(x)           # (B, F, T)\n",
    "        x = x.permute(0, 2, 1)       # (B, T, F)\n",
    "        if self.projection:\n",
    "            x = self.projection(x)   # (B, T, D)\n",
    "        return x                     # (B, T, embedding_dim)\n",
    "\n",
    "\n",
    "class SVDPPMIEmbedding(nn.Module):\n",
    "    \"\"\"Build PPMI→SVD char embeddings, then (if needed) project to embedding_dim.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seqs: List[List[int]],\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        window: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 1) Build co-occurrence counts\n",
    "        cooc = np.zeros((vocab_size, vocab_size), dtype=np.float64)\n",
    "        for seq in token_seqs:\n",
    "            for i, u in enumerate(seq):\n",
    "                if u == padding_idx: continue\n",
    "                for j in range(max(0, i - window), min(len(seq), i + window + 1)):\n",
    "                    if i == j: continue\n",
    "                    v = seq[j]\n",
    "                    if v == padding_idx: continue\n",
    "                    cooc[u, v] += 1\n",
    "\n",
    "        # 2) Compute PPMI matrix\n",
    "        total = cooc.sum()\n",
    "        row_sums = cooc.sum(axis=1, keepdims=True)\n",
    "        col_sums = cooc.sum(axis=0, keepdims=True)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            pmi = np.log((cooc * total) / (row_sums * col_sums))\n",
    "        pmi[np.isnan(pmi)] = 0.0\n",
    "        pmi[pmi < 0] = 0.0\n",
    "\n",
    "        # 3) Truncated SVD\n",
    "        U, S, _ = np.linalg.svd(pmi, full_matrices=False)\n",
    "        # D0 is the actual SVD dimension we get (≤ vocab_size)\n",
    "        D0 = min(embedding_dim, U.shape[1])\n",
    "        U = U[:, :D0]            # (vocab_size, D0)\n",
    "        S = S[:D0]               # (D0,)\n",
    "        emb_matrix = U * np.sqrt(S)  # (vocab_size, D0)\n",
    "\n",
    "        # Register static SVD weights\n",
    "        self.register_buffer(\"weight\", torch.from_numpy(emb_matrix).float())\n",
    "\n",
    "        # 4) If the SVD rank D0 is smaller than requested embedding_dim, add a projection\n",
    "        if D0 < embedding_dim:\n",
    "            # Warn the user clearly\n",
    "            warnings.warn(\n",
    "                f\"SVD/PPMI yielded only {D0} dimensions (≤ vocab size), \"\n",
    "                f\"but embedding_size={embedding_dim} was requested. \"\n",
    "                \"Adding a learnable linear projection to expand from \"\n",
    "                f\"{D0} → {embedding_dim} dimensions.\",\n",
    "                UserWarning\n",
    "            )\n",
    "            self.expander = nn.Linear(D0, embedding_dim, bias=False)\n",
    "        else:\n",
    "            self.expander = None\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # Lookup static SVD embeddings → (B, T, D0)\n",
    "        x = F.embedding(token_ids, self.weight, padding_idx=self.weight.new_zeros(1).long())\n",
    "        # If we have an expander, project to the full embedding_dim\n",
    "        if self.expander:\n",
    "            x = self.expander(x)\n",
    "        return x  # (B, T, embedding_dim)\n",
    "\n",
    "\n",
    "# ───────────────────────── 3. Seq2SeqConfig ─────────────────────────\n",
    "@dataclass\n",
    "class Seq2SeqConfig:\n",
    "    \"\"\"Holds hyper-parameters and options for the Seq2Seq model.\"\"\"\n",
    "    # mandatory vocab sizes\n",
    "    source_vocab_size: int\n",
    "    target_vocab_size: int\n",
    "\n",
    "    # embedding / hidden dims\n",
    "    embedding_dim: int = 256\n",
    "    hidden_dim: int = 512\n",
    "\n",
    "    # encoder/decoder depth & type\n",
    "    encoder_layers: int = 2\n",
    "    decoder_layers: int = 2\n",
    "    cell_type: str = \"LSTM\"  # choices: RNN | LSTM | GRU\n",
    "\n",
    "    # dropout for multi-layer RNNs\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # special token indices\n",
    "    pad_index: int = SPECIAL_TOKENS[\"<pad>\"]\n",
    "    sos_index: int = SPECIAL_TOKENS[\"<sos>\"]\n",
    "    eos_index: int = SPECIAL_TOKENS[\"<eos>\"]\n",
    "\n",
    "    # which character embedding method to use\n",
    "    embedding_method: str = \"learned\"  # learned | onehot | char_cnn | svd_ppmi\n",
    "\n",
    "    # only used if embedding_method == \"svd_ppmi\"\n",
    "    svd_sources: Optional[List[List[int]]] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.cell_type in {\"RNN\", \"LSTM\", \"GRU\"}, \"cell_type must be RNN, LSTM or GRU\"\n",
    "        assert self.embedding_method in {\"learned\", \"onehot\", \"char_cnn\", \"svd_ppmi\"}, (\n",
    "            \"embedding_method must be one of learned | onehot | char_cnn | svd_ppmi\"\n",
    "        )\n",
    "        if self.embedding_method == \"svd_ppmi\":\n",
    "            assert self.svd_sources is not None, \"svd_ppmi requires svd_sources\"\n",
    "\n",
    "\n",
    "# map a string to the corresponding nn.RNN module\n",
    "_RNN_MAP: Dict[str, nn.Module] = {\n",
    "    \"RNN\": nn.RNN,\n",
    "    \"LSTM\": nn.LSTM,\n",
    "    \"GRU\": nn.GRU,\n",
    "}\n",
    "\n",
    "# ─────────────────────────── Helper: Align Hidden State ──────────────────────────\n",
    "def _align_hidden_state(hidden_state, target_num_layers: int):\n",
    "    \"\"\"\n",
    "    Adjust the encoder's final hidden state to match the decoder's expected\n",
    "    number of layers. Works for both LSTM (tuple of (h,c)) and GRU/RNN (single tensor).\n",
    "    \n",
    "    Strategies:\n",
    "    - If encoder_layers == decoder_layers: return hidden_state unchanged.\n",
    "    - If encoder_layers  > decoder_layers: take the **last** `target_num_layers` layers.\n",
    "    - If encoder_layers  < decoder_layers: **repeat** the final layer's state\n",
    "      so that the total number of layers equals `target_num_layers`.\n",
    "    \"\"\"\n",
    "    def _repeat_last_layer(tensor, repeat_count: int):\n",
    "        # tensor shape: (enc_layers, batch_size, hidden_dim)\n",
    "        last_layer = tensor[-1:]                             # shape: (1, B, H)\n",
    "        repeated   = last_layer.expand(repeat_count, -1, -1) # shape: (repeat_count, B, H)\n",
    "        return torch.cat([tensor, repeated], dim=0)          # new shape: (enc_layers+repeat_count, B, H)\n",
    "\n",
    "    if isinstance(hidden_state, tuple):\n",
    "        h, c = hidden_state\n",
    "        enc_layers, batch_size, hid_dim = h.shape\n",
    "\n",
    "        if enc_layers == target_num_layers:\n",
    "            return h, c\n",
    "        \n",
    "        # Warn whenever we need to truncate or repeat\n",
    "        warnings.warn(\n",
    "            f\"Encoder has {enc_layers} layers but decoder expects {target_num_layers}. \"\n",
    "            f\"{'Truncating' if enc_layers>target_num_layers else 'Repeating last layer'} hidden state.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "        if enc_layers > target_num_layers:\n",
    "            # keep only the last `target_num_layers` layers\n",
    "            return h[-target_num_layers:], c[-target_num_layers:]\n",
    "        else:\n",
    "            # repeat the final layer's state to pad up to target_num_layers\n",
    "            to_repeat = target_num_layers - enc_layers\n",
    "            return _repeat_last_layer(h, to_repeat), _repeat_last_layer(c, to_repeat)\n",
    "    else:\n",
    "        h = hidden_state\n",
    "        enc_layers, batch_size, hid_dim = h.shape\n",
    "\n",
    "        if enc_layers == target_num_layers:\n",
    "            return h\n",
    "        \n",
    "        # Same warning for single‐tensor hidden states\n",
    "        warnings.warn(\n",
    "            f\"Encoder has {enc_layers} layers but decoder expects {target_num_layers}. \"\n",
    "            f\"{'Truncating' if enc_layers>target_num_layers else 'Repeating last layer'} hidden state.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "        if enc_layers > target_num_layers:\n",
    "            return h[-target_num_layers:]\n",
    "        else:\n",
    "            to_repeat = target_num_layers - enc_layers\n",
    "            return _repeat_last_layer(h, to_repeat)\n",
    "\n",
    "\n",
    "# ───────────────────────── 4. Encoder ─────────────────────────\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder: character embeddings → RNN → hidden state(s).\"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # choose between various embedding modules\n",
    "        if cfg.embedding_method == \"learned\":\n",
    "            self.embedding = nn.Embedding(\n",
    "                cfg.source_vocab_size,\n",
    "                cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "        elif cfg.embedding_method == \"onehot\":\n",
    "            self.embedding = OneHotEmbedding(\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "        elif cfg.embedding_method == \"char_cnn\":\n",
    "            self.embedding = CharCNNEmbedding(\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index,\n",
    "                kernel_size=3\n",
    "            )\n",
    "        elif cfg.embedding_method == \"svd_ppmi\":\n",
    "            self.embedding = SVDPPMIEmbedding(\n",
    "                token_seqs=cfg.svd_sources,\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index,\n",
    "                window=2\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding_method {cfg.embedding_method}\")\n",
    "\n",
    "        # set up the RNN cell\n",
    "        rnn_cls = _RNN_MAP[cfg.cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=cfg.embedding_dim,\n",
    "            hidden_size=cfg.hidden_dim,\n",
    "            num_layers=cfg.encoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=cfg.dropout if cfg.encoder_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_lengths: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src         – LongTensor (B, T_src)\n",
    "            src_lengths – LongTensor (B,) true lengths before padding\n",
    "        Returns:\n",
    "            hidden_state – (h, c) for LSTM or h for GRU/RNN\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(src)  # (B, T_src, D)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, hidden_state = self.rnn(packed)\n",
    "        return hidden_state  # LSTM → (h,c), GRU/RNN → h\n",
    "\n",
    "\n",
    "# ───────────────────────── 5. Decoder ─────────────────────────\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder: one step of embedding → RNN → vocab logits.\"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embedding = nn.Embedding(\n",
    "            cfg.target_vocab_size,\n",
    "            cfg.embedding_dim,\n",
    "            padding_idx=cfg.pad_index\n",
    "        )\n",
    "        rnn_cls = _RNN_MAP[cfg.cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            cfg.embedding_dim,\n",
    "            cfg.hidden_dim,\n",
    "            num_layers=cfg.decoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=cfg.dropout if cfg.decoder_layers > 1 else 0.0\n",
    "        )\n",
    "        self.to_vocab_logits = nn.Linear(cfg.hidden_dim, cfg.target_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        current_input: torch.LongTensor,\n",
    "        hidden_state\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor] | torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          current_input – LongTensor (B, 1)\n",
    "          hidden_state  – previous hidden state(s)\n",
    "        Returns:\n",
    "          logits        – (B, 1, V)\n",
    "          hidden_state  – updated hidden state(s)\n",
    "        \"\"\"\n",
    "        emb = self.embedding(current_input)            # (B,1,D)\n",
    "        output, new_hidden = self.rnn(emb, hidden_state)  # (B,1,H)\n",
    "        logits = self.to_vocab_logits(output)          # (B,1,V)\n",
    "        return logits, new_hidden\n",
    "\n",
    "\n",
    "# ───────────────────────── 6. Seq2Seq wrapper ─────────────────────────\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Flexible encoder-decoder wrapper combining Encoder and Decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Seq2SeqConfig):\n",
    "        super().__init__()\n",
    "        self.cfg     = cfg\n",
    "        self.encoder = Encoder(cfg)\n",
    "        self.decoder = Decoder(cfg)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    # Training-time forward (teacher forcing)\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor,\n",
    "        tgt: torch.LongTensor, *,\n",
    "        teacher_forcing_ratio: float = 0.5,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute logits for each timestep in the target sequence using teacher forcing.\n",
    "        Returns logits_all of shape (B, T_tgt, vocab_size).\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        logits_all = torch.zeros(batch_size, tgt_len, self.cfg.target_vocab_size, device=tgt.device)\n",
    "\n",
    "        # 1) Encode source\n",
    "        hidden_state = self.encoder(src, src_lengths)\n",
    "\n",
    "        # 2) Align hidden_state to decoder depth\n",
    "        hidden_state = _align_hidden_state(hidden_state, self.cfg.decoder_layers)\n",
    "\n",
    "        # 3) First decoder input is <sos>\n",
    "        decoder_input = tgt[:, 0].unsqueeze(1)  # (B,1)\n",
    "\n",
    "        # 4) Unroll for each timestep\n",
    "        for t in range(1, tgt_len):\n",
    "            step_logits, hidden_state = self.decoder(decoder_input, hidden_state)\n",
    "            logits_all[:, t] = step_logits.squeeze(1)\n",
    "            # decide next input\n",
    "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                decoder_input = tgt[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = step_logits.argmax(-1)\n",
    "\n",
    "        return logits_all\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    # Greedy decoding\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    def greedy_decode(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor, *,\n",
    "        max_len: int = 50,\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Greedy decoding for inference: always take argmax.\n",
    "        Returns tensor of shape (B, <=max_len).\n",
    "        \"\"\"\n",
    "        B = src.size(0)\n",
    "        hidden_state = self.encoder(src, src_lengths)\n",
    "        hidden_state = _align_hidden_state(hidden_state, self.cfg.decoder_layers)\n",
    "\n",
    "        decoder_input = torch.full(\n",
    "            (B, 1),\n",
    "            self.cfg.sos_index,\n",
    "            dtype=torch.long,\n",
    "            device=src.device\n",
    "        )\n",
    "        generated_ids: List[torch.LongTensor] = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            step_logits, hidden_state = self.decoder(decoder_input, hidden_state)\n",
    "            next_ids = step_logits.argmax(-1)  # (B,1)\n",
    "            generated_ids.append(next_ids)\n",
    "            decoder_input = next_ids\n",
    "\n",
    "        return torch.cat(generated_ids, dim=1)  # (B, seq_len)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    # Beam-search decoding\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    def beam_search_decode(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor, *,\n",
    "        beam_size: int = 5,\n",
    "        max_len: int = 50,\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Beam search decoding (only batch_size=1 supported).\n",
    "        Returns the best sequence of token ids as a tensor of shape (1, <=max_len).\n",
    "        \"\"\"\n",
    "        B = src.size(0)\n",
    "        assert B == 1, \"beam_search_decode currently only supports batch_size=1\"\n",
    "\n",
    "        # 1) Encode and align\n",
    "        hidden_state = self.encoder(src, src_lengths)\n",
    "        hidden_state = _align_hidden_state(hidden_state, self.cfg.decoder_layers)\n",
    "\n",
    "        # Initialize beams: each is (sequence_list, cumulative_log_prob, hidden_state)\n",
    "        beams = [([self.cfg.sos_index], 0.0, hidden_state)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            candidates: List[Tuple[List[int], float, Any]] = []\n",
    "            for seq, score, h_state in beams:\n",
    "                last_token = seq[-1]\n",
    "                # If already ended with <eos>, carry forward unchanged\n",
    "                if last_token == self.cfg.eos_index:\n",
    "                    candidates.append((seq, score, h_state))\n",
    "                    continue\n",
    "\n",
    "                # Run one decoder step\n",
    "                inp = torch.tensor([[last_token]], device=src.device)\n",
    "                logits, new_hidden = self.decoder(inp, h_state)  # (1,1,V)\n",
    "                log_probs = torch.log_softmax(logits.squeeze(1), dim=-1)  # (V,)\n",
    "\n",
    "                # Expand top-k continuations\n",
    "                topk_vals, topk_idx = log_probs.topk(beam_size)\n",
    "                for log_p, idx in zip(topk_vals.tolist(), topk_idx.tolist()):\n",
    "                    candidates.append((seq + [idx], score + log_p, new_hidden))\n",
    "\n",
    "            # Prune back to top beam_size\n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "        # Choose best final beam\n",
    "        best_seq, best_score, _ = max(beams, key=lambda x: x[1])\n",
    "        return torch.tensor(best_seq, dtype=torch.long, device=src.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────── 7. Training & evaluation ─────────────────────────\n",
    "def train_epoch(\n",
    "    model: Seq2Seq,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    device: str,\n",
    "    teacher_forcing: float\n",
    ") -> float:\n",
    "    \"\"\"Run one epoch of training; return average token-level cross-entropy loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for src, src_len, tgt in loader:\n",
    "        src, src_len, tgt = src.to(device), src_len.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, src_len, tgt, teacher_forcing_ratio=teacher_forcing)\n",
    "        # shift tgt so we predict t when truth is at t\n",
    "        gold = tgt[:, 1:].contiguous()\n",
    "        preds = logits[:, 1:].contiguous()\n",
    "        loss = loss_fn(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_valid = (gold != loss_fn.ignore_index).sum().item()\n",
    "        total_loss += loss.item() * n_valid\n",
    "        total_tokens += n_valid\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def eval_epoch(\n",
    "    model: Seq2Seq,\n",
    "    loader: DataLoader,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    device: str\n",
    ") -> float:\n",
    "    \"\"\"Run one epoch of evaluation; return average token-level cross-entropy loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_len, tgt in loader:\n",
    "            src, src_len, tgt = src.to(device), src_len.to(device), tgt.to(device)\n",
    "            logits = model(src, src_len, tgt, teacher_forcing_ratio=0.0)\n",
    "            gold = tgt[:, 1:].contiguous()\n",
    "            preds = logits[:, 1:].contiguous()\n",
    "            loss = loss_fn(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "            n_valid = (gold != loss_fn.ignore_index).sum().item()\n",
    "            total_loss += loss.item() * n_valid\n",
    "            total_tokens += n_valid\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "# ───────────────────────── 8. Argument parsing ─────────────────────────\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Train Seq2Seq on the Dakshina Hindi lexicon (character-level transliteration)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--train_tsv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the training lexicon TSV (columns: native_word, romanized_word, count)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--dev_tsv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the development (validation) lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--test_tsv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the test lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Number of full passes over the training data\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of examples per mini-batch\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--embedding_size\",\n",
    "        type=int,\n",
    "        default=256,\n",
    "        help=\"Dimensionality of the character embedding vectors\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--hidden_size\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Size of the hidden states in the encoder and decoder RNNs\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--encoder_layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Number of stacked RNN layers in the encoder\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--decoder_layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Number of stacked RNN layers in the decoder\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--cell\",\n",
    "        choices=[\"RNN\", \"LSTM\", \"GRU\"],\n",
    "        default=\"LSTM\",\n",
    "        help=\"Type of RNN cell to use for both encoder and decoder\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help=\"Learning rate for the Adam optimizer\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--teacher_forcing\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Probability of using teacher forcing at each decoder time step (0.0–1.0)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=DEFAULT_DEVICE,\n",
    "        help=\"Computation device: 'cpu' or 'cuda' (default automatically detects GPU if available)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--embedding_method\",\n",
    "        choices=[\"learned\", \"onehot\", \"char_cnn\", \"svd_ppmi\"],\n",
    "        default=\"learned\",\n",
    "        help=(\n",
    "            \"How to convert characters to vectors: \"\n",
    "            \"'learned' lookup (default), explicit 'onehot'+linear, \"\n",
    "            \"'char_cnn' for a 1D CNN over one-hots, or 'svd_ppmi' for \"\n",
    "            \"static SVD over PPMI co-occurrences\"\n",
    "        )\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--use_attestations\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If set, sample training examples proportional to their annotation count\"\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "# ─────────────────────────── 9. Main driver ───────────────────────────\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # ─── Prepare datasets ───────────────────────────────────────────────\n",
    "    # Train set: build vocabularies and optionally keep counts\n",
    "    train_ds = DakshinaLexicon(\n",
    "        args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=args.use_attestations\n",
    "    )\n",
    "    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n",
    "\n",
    "    # Dev/test: reuse the same vocabs (counts not needed)\n",
    "    dev_ds  = DakshinaLexicon(args.dev_tsv,  src_vocab, tgt_vocab)\n",
    "    test_ds = DakshinaLexicon(args.test_tsv, src_vocab, tgt_vocab)\n",
    "\n",
    "    # Collate function for padding\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "\n",
    "    # Training loader: either shuffle or sample by counts\n",
    "    if args.use_attestations:\n",
    "        assert train_ds.example_counts is not None, \"Attestations requested but counts missing\"\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=train_ds.example_counts,\n",
    "            num_samples=len(train_ds),\n",
    "            replacement=True\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    dev_loader  = DataLoader(dev_ds,  batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # ─── Build model ────────────────────────────────────────────────────\n",
    "    extra_cfg = {}\n",
    "    if args.embedding_method == \"svd_ppmi\":\n",
    "        # supply the token sequences for PPMI/SVD\n",
    "        extra_cfg[\"svd_sources\"] = train_ds.encoded_sources\n",
    "\n",
    "    cfg = Seq2SeqConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=args.embedding_size,\n",
    "        hidden_dim=args.hidden_size,\n",
    "        encoder_layers=args.encoder_layers,\n",
    "        decoder_layers=args.decoder_layers,\n",
    "        cell_type=args.cell,\n",
    "        dropout=0.1,\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=args.embedding_method,\n",
    "        **extra_cfg\n",
    "    )\n",
    "    model = Seq2Seq(cfg).to(args.device)\n",
    "\n",
    "    # Optimizer & loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    # ─── Training loop ───────────────────────────────────────────────────\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, args.device, args.teacher_forcing)\n",
    "        dev_loss   = eval_epoch(model, dev_loader, loss_fn, args.device)\n",
    "        print(f\"Epoch {epoch:02d} | train ppl={math.exp(train_loss):.2f} | dev ppl={math.exp(dev_loss):.2f}\")\n",
    "\n",
    "    # ─── Final evaluation on test ────────────────────────────────────────\n",
    "    test_loss = eval_epoch(model, test_loader, loss_fn, args.device)\n",
    "    print(f\"Test perplexity: {math.exp(test_loss):.2f}\")\n",
    "\n",
    "    # ─── Qualitative samples ─────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    print(\"Sample dev-set transliterations:\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(5):\n",
    "            src_ids, tgt_ids = dev_ds[i]\n",
    "            src_len = len(src_ids)\n",
    "            src_tensor = torch.tensor([src_ids], device=args.device)\n",
    "            len_tensor = torch.tensor([src_len], device=args.device)\n",
    "\n",
    "            pred_ids = model.greedy_decode(src_tensor, len_tensor, max_len=30)[0].tolist()\n",
    "            romanized = src_vocab.decode(src_ids)\n",
    "            gold       = tgt_vocab.decode(tgt_ids[1:])  # skip <sos>\n",
    "            pred_str   = tgt_vocab.decode(pred_ids)\n",
    "            print(f\"{romanized:15} → {pred_str:15} (gold: {gold})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13551e",
   "metadata": {},
   "source": [
    "# Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q2: W&B sweep driver for character-level Hindi transliteration\n",
    "────────────────────────────────────────────────────────────────\n",
    "Re-uses the full Seq2Seq implementation from solution_1.py (Q1).\n",
    "A YAML file under ./configs/ specifies the sweep search space.\n",
    "\n",
    "Example usage\n",
    "~~~~~~~~~~~~~\n",
    "# 1) Create the sweep and directly launch the agent:\n",
    "python solution_2.py \\\n",
    "    --mode sweep \\\n",
    "    --sweep_config sweep_config.yaml \\\n",
    "    --wandb_project transliteration \\\n",
    "    --wandb_entity your_entity \\\n",
    "    --wandb_run_tag baseline \\\n",
    "    --gpu_ids 0 1 \\\n",
    "    --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "    --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "    --test_tsv  ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "    --sweep_count 30\n",
    "\n",
    "# 2) For a single debug run:\n",
    "python solution_2.py \\\n",
    "    --mode single \\\n",
    "    --wandb_project transliteration \\\n",
    "    --wandb_entity your_entity \\\n",
    "    --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "    --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "    --test_tsv  ./lexicons/hi.translit.sampled.test.tsv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ───────────────────────── Imports ─────────────────────────\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Import Q1 implementation (updated solution_1.py)\n",
    "from solution_1 import (\n",
    "    DakshinaLexicon,\n",
    "    CharVocabulary,\n",
    "    Seq2SeqConfig,\n",
    "    Seq2Seq,\n",
    "    collate_batch,\n",
    "    train_epoch,\n",
    "    eval_epoch,\n",
    ")\n",
    "\n",
    "# ────────────── 1. YAML helper requested by the user ──────────────\n",
    "def get_configs(project_root: str | Path, config_filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load a YAML sweep configuration from ./configs/ relative to project_root.\n",
    "    \"\"\"\n",
    "    cfg_path = Path(project_root) / \"configs\" / config_filename\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        config: Dict[str, Any] = yaml.safe_load(handle)\n",
    "    return config\n",
    "\n",
    "\n",
    "# ────────────── 2. A single training run (used by sweep) ──────────────\n",
    "def run_single_training(sweep_config: Dict[str, Any], static_args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Train + evaluate once using the hyper-parameters in sweep_config and\n",
    "    the static file paths / tag supplied via CLI.\n",
    "    \"\"\"\n",
    "    # ─── Honour multi-GPU pinning via CUDA_VISIBLE_DEVICES ─────────────────\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(g) for g in static_args.gpu_ids)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ─── Warn if encoder/decoder depths differ ─────────────────────────────\n",
    "    if sweep_config[\"encoder_layers\"] != sweep_config[\"decoder_layers\"]:\n",
    "        warnings.warn(\n",
    "            f\"Encoder layers ({sweep_config['encoder_layers']}) != \"\n",
    "            f\"decoder layers ({sweep_config['decoder_layers']}), \"\n",
    "            \"hidden states will be aligned automatically.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "    # ─── Data loading ────────────────────────────────────────────────\n",
    "    train_dataset = DakshinaLexicon(\n",
    "        static_args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=sweep_config.get(\"use_attestations\", False),\n",
    "    )\n",
    "    src_vocab: CharVocabulary = train_dataset.src_vocab\n",
    "    tgt_vocab: CharVocabulary = train_dataset.tgt_vocab\n",
    "\n",
    "    dev_dataset  = DakshinaLexicon(static_args.dev_tsv,  src_vocab, tgt_vocab)\n",
    "    test_dataset = DakshinaLexicon(static_args.test_tsv, src_vocab, tgt_vocab)\n",
    "\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=sweep_config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    dev_loader  = torch.utils.data.DataLoader(\n",
    "        dev_dataset,\n",
    "        batch_size=sweep_config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=sweep_config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # ─── Build the Seq2Seq model ─────────────────────────────────────────\n",
    "    extra_cfg: Dict[str, Any] = {}\n",
    "    if sweep_config[\"embedding_method\"] == \"svd_ppmi\":\n",
    "        extra_cfg[\"svd_sources\"] = train_dataset.encoded_sources\n",
    "\n",
    "    model_cfg = Seq2SeqConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=sweep_config[\"embedding_size\"],\n",
    "        hidden_dim=sweep_config[\"hidden_size\"],\n",
    "        encoder_layers=sweep_config[\"encoder_layers\"],\n",
    "        decoder_layers=sweep_config[\"decoder_layers\"],\n",
    "        cell_type=sweep_config[\"cell\"],\n",
    "        dropout=sweep_config[\"dropout\"],\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=sweep_config[\"embedding_method\"],\n",
    "        **extra_cfg,\n",
    "    )\n",
    "    model = Seq2Seq(model_cfg)\n",
    "\n",
    "    # ─── Wrap in DataParallel if multiple GPUs specified ─────────────────\n",
    "    if device == \"cuda\" and len(static_args.gpu_ids) > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=sweep_config[\"learning_rate\"])\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=model_cfg.pad_index)\n",
    "\n",
    "    # ─── Unique, human-readable run name ─────────────────────────────\n",
    "    run_name = (\n",
    "        f\"emb:{sweep_config['embedding_method']}-{sweep_config['embedding_size']}|\"\n",
    "        f\"cell:{sweep_config['cell']}|hid:{sweep_config['hidden_size']}|\"\n",
    "        f\"enc:{sweep_config['encoder_layers']}|dec:{sweep_config['decoder_layers']}|\"\n",
    "        f\"dr:{sweep_config['dropout']}|lr:{sweep_config['learning_rate']}|\"\n",
    "        f\"bs:{sweep_config['batch_size']}|tf:{sweep_config['teacher_forcing']}|\"\n",
    "        f\"ep:{sweep_config['epochs']}|beam:{sweep_config.get('beam_size',1)}\"\n",
    "    )\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.tags = [static_args.wandb_run_tag]\n",
    "\n",
    "    # ─── Training loop ───────────────────────────────────────────────\n",
    "    for epoch in range(1, sweep_config[\"epochs\"] + 1):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss_fn, device, sweep_config[\"teacher_forcing\"]\n",
    "        )\n",
    "        dev_loss = eval_epoch(model, dev_loader, loss_fn, device)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_perplexity\": math.exp(train_loss),\n",
    "            \"dev_perplexity\":   math.exp(dev_loss),\n",
    "        })\n",
    "\n",
    "    # ─── Final test evaluation ──────────────────────────────────────\n",
    "    test_loss = eval_epoch(model, test_loader, loss_fn, device)\n",
    "    wandb.log({\"test_perplexity\": math.exp(test_loss)})\n",
    "\n",
    "    # ─── Qualitative beam-search samples ─────────────────────────────\n",
    "    beam = sweep_config.get(\"beam_size\", 1)\n",
    "    print(f\"\\nSample dev-set translations (beam_size={beam}):\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(5):\n",
    "            src_ids, tgt_ids = dev_dataset[i]  # dev_dataset returns (src, tgt)\n",
    "            src_len = len(src_ids)\n",
    "            src_tensor = torch.tensor([src_ids], device=device)\n",
    "            len_tensor = torch.tensor([src_len], device=device)\n",
    "\n",
    "            # choose beam or greedy\n",
    "            if beam > 1:\n",
    "                pred_ids = model.beam_search_decode(\n",
    "                    src_tensor, len_tensor,\n",
    "                    beam_size=beam,\n",
    "                    max_len=30\n",
    "                )[0].tolist()\n",
    "            else:\n",
    "                pred_ids = model.greedy_decode(\n",
    "                    src_tensor, len_tensor,\n",
    "                    max_len=30\n",
    "                )[0].tolist()\n",
    "\n",
    "            romanized = src_vocab.decode(src_ids)\n",
    "            gold       = tgt_vocab.decode(tgt_ids[1:])  # skip <sos>\n",
    "            prediction = tgt_vocab.decode(pred_ids)\n",
    "            print(f\"{romanized:15} → {prediction:15} (gold: {gold})\")\n",
    "\n",
    "\n",
    "# ────────────── 3. Sweep & CLI plumbing ──────────────\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Launch or run a W&B sweep for Q2.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mode\", choices=[\"sweep\", \"single\"], required=True,\n",
    "        help=\"'sweep' to create & launch the sweep; 'single' for debug run\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sweep_config\", type=str, default=\"sweep_config.yaml\",\n",
    "        help=\"YAML filename under ./configs/ defining the sweep space\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_project\", type=str, required=True, help=\"W&B project name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_entity\",  type=str, required=True, help=\"W&B entity (team/user)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_run_tag\", type=str, default=\"baseline\",\n",
    "        help=\"Static tag added to every W&B run\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gpu_ids\", type=int, nargs=\"+\", default=[0],\n",
    "        help=\"CUDA device IDs to use (e.g. --gpu_ids 0 1 for two GPUs)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_tsv\", type=str, required=True, help=\"Path to train TSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dev_tsv\",   type=str, required=True, help=\"Path to dev TSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_tsv\",  type=str, required=True, help=\"Path to test TSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sweep_count\", type=int, default=30,\n",
    "        help=\"Number of sweep runs to launch if mode==sweep\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load sweep_config YAML\n",
    "    project_root = Path(__file__).resolve().parent\n",
    "    sweep_yaml   = get_configs(project_root, args.sweep_config)\n",
    "\n",
    "    if args.mode == \"sweep\":\n",
    "        # ─── Prepare sweep definition ─────────────────────────────────\n",
    "        sweep_yaml.setdefault(\"method\", \"bayes\")\n",
    "        sweep_yaml[\"program\"]    = Path(__file__).name\n",
    "        sweep_yaml.setdefault(\n",
    "            \"metric\", {\"name\": \"dev_perplexity\", \"goal\": \"minimize\"}\n",
    "        )\n",
    "        sweep_yaml.setdefault(\"parameters\", {})\n",
    "        sweep_yaml[\"run_cap\"]    = args.sweep_count\n",
    "\n",
    "        # ─── Register the sweep with W&B ───────────────────────────────\n",
    "        sweep_id = wandb.sweep(\n",
    "            sweep=sweep_yaml,\n",
    "            project=args.wandb_project,\n",
    "            entity=args.wandb_entity,\n",
    "        )\n",
    "        print(f\"Registered sweep with id: {sweep_id}\")\n",
    "\n",
    "        # ─── Define the function the agent will call for each trial ────\n",
    "        def sweep_train():\n",
    "            run_single_training(dict(wandb.config), args)\n",
    "\n",
    "        # ─── Launch W&B agent in-process ───────────────────────────────\n",
    "        print(f\"Launching W&B agent for sweep {sweep_id}, count={args.sweep_count}\")\n",
    "        wandb.agent(\n",
    "            sweep_id,\n",
    "            function=sweep_train,\n",
    "            count=args.sweep_count\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # ─── Single run for debugging ───────────────────────────────────\n",
    "        with wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            entity=args.wandb_entity,\n",
    "            config=sweep_yaml.get(\"parameters\", {}),\n",
    "        ) as run:\n",
    "            run.config.update(\n",
    "                {\n",
    "                    \"epochs\": 3,\n",
    "                    \"batch_size\": 64,\n",
    "                    \"embedding_size\": 128,\n",
    "                    \"hidden_size\": 256,\n",
    "                    \"encoder_layers\": 2,\n",
    "                    \"decoder_layers\": 2,\n",
    "                    \"cell\": \"LSTM\",\n",
    "                    \"dropout\": 0.1,\n",
    "                    \"learning_rate\": 1e-3,\n",
    "                    \"teacher_forcing\": 0.5,\n",
    "                    \"embedding_method\": \"learned\",\n",
    "                    \"use_attestations\": False,\n",
    "                    \"beam_size\": 1,\n",
    "                },\n",
    "                allow_val_change=True,\n",
    "            )\n",
    "            run_single_training(dict(run.config), args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
