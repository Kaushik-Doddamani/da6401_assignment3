{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3980435c",
   "metadata": {},
   "source": [
    "# Q1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end training script for Q1 on the Dakshina Hindi lexicon.\n",
    "\n",
    "This script:\n",
    "  1. Loads and preprocesses the Hindi transliteration lexicon into PyTorch datasets.\n",
    "  2. Builds a flexible Seq2Seq model with configurable embedding size, hidden size,\n",
    "     cell type (RNN/LSTM/GRU), number of layers, and choice of character‐vector methods.\n",
    "  3. Trains the model with teacher forcing (optionally sampling by attestation counts).\n",
    "  4. Evaluates on dev/test sets and prints perplexities.\n",
    "  5. Runs a few qualitative transliteration examples.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "    python train_dakshina_seq2seq.py \\\n",
    "      --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "      --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv   \\\n",
    "      --test_tsv  ./lexicons/hi.translit.sampled.test.tsv  \\\n",
    "      --embedding_size 256 \\\n",
    "      --hidden_size    512 \\\n",
    "      --encoder_layers 2 \\\n",
    "      --decoder_layers 2 \\\n",
    "      --cell LSTM \\\n",
    "      --epochs 15 \\\n",
    "      --embedding_method svd_ppmi \\\n",
    "      --use_attestations\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "# Use GPU if available\n",
    "DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ─────────────────────── 1. Vocabulary helpers ───────────────────────\n",
    "SPECIAL_TOKENS = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
    "\n",
    "\n",
    "class CharVocabulary:\n",
    "    \"\"\"Character-level vocabulary that handles <pad>, <sos>, and <eos>.\"\"\"\n",
    "    def __init__(self, characters: List[str]):\n",
    "        unique_chars = sorted(set(characters))\n",
    "        # string→index and index→string maps\n",
    "        self.stoi: Dict[str, int] = {\n",
    "            **SPECIAL_TOKENS,\n",
    "            **{ch: idx + len(SPECIAL_TOKENS) for idx, ch in enumerate(unique_chars)}\n",
    "        }\n",
    "        self.itos: Dict[int, str] = {idx: ch for ch, idx in self.stoi.items()}\n",
    "\n",
    "    def encode(self, text: str, *, add_sos: bool = False, add_eos: bool = True) -> List[int]:\n",
    "        \"\"\"Convert a string to list of token ids (with optional <sos>/<eos>).\"\"\"\n",
    "        ids = [self.stoi[ch] for ch in text]\n",
    "        if add_eos:\n",
    "            ids.append(self.stoi[\"<eos>\"])\n",
    "        if add_sos:\n",
    "            ids.insert(0, self.stoi[\"<sos>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert ids back to string (stop at <eos>).\"\"\"\n",
    "        chars: List[str] = []\n",
    "        for idx in ids:\n",
    "            if idx == self.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            chars.append(self.itos.get(idx, \"\"))\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of tokens in vocabulary.\"\"\"\n",
    "        return len(self.stoi)\n",
    "\n",
    "\n",
    "# ───────────────────────── 2. Dataset class ─────────────────────────\n",
    "class DakshinaLexicon(Dataset):\n",
    "    \"\"\"Loads a Dakshina *lexicon* TSV and encodes (source, target) pairs.\n",
    "\n",
    "    TSV columns: native_word, romanized_word, count\n",
    "    We treat romanized_word as source and native_word as target.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tsv_path: str | Path,\n",
    "        source_vocab: Optional[CharVocabulary] = None,\n",
    "        target_vocab: Optional[CharVocabulary] = None,\n",
    "        *,\n",
    "        build_vocabs: bool = False,\n",
    "        use_attestations: bool = False\n",
    "    ):\n",
    "        # Read TSV – three columns, ensure correct dtypes\n",
    "        dataframe = pd.read_csv(\n",
    "            tsv_path, sep=\"\\t\", header=None,\n",
    "            names=[\"target_native\", \"source_roman\", \"count\"],\n",
    "            dtype={\"target_native\": str, \"source_roman\": str, \"count\": int}\n",
    "        ).dropna()\n",
    "\n",
    "        # Optionally keep annotator counts for sampling or weighting\n",
    "        self.example_counts: Optional[List[int]] = (\n",
    "            dataframe[\"count\"].tolist() if use_attestations else None\n",
    "        )\n",
    "\n",
    "        # Keep only the (src, tgt) pairs\n",
    "        self.word_pairs: List[Tuple[str, str]] = list(zip(\n",
    "            dataframe[\"source_roman\"], dataframe[\"target_native\"]\n",
    "        ))\n",
    "\n",
    "        # Build new or reuse provided vocabularies\n",
    "        if build_vocabs:\n",
    "            assert source_vocab is None and target_vocab is None, (\n",
    "                \"Cannot pass existing vocabs when build_vocabs=True\"\n",
    "            )\n",
    "            # collect all chars\n",
    "            all_src_chars = [ch for src, _ in self.word_pairs for ch in src]\n",
    "            all_tgt_chars = [ch for _, tgt in self.word_pairs for ch in tgt]\n",
    "            source_vocab = CharVocabulary(all_src_chars)\n",
    "            target_vocab = CharVocabulary(all_tgt_chars)\n",
    "\n",
    "        assert source_vocab is not None and target_vocab is not None, (\n",
    "            \"Must provide or build vocabularies\"\n",
    "        )\n",
    "        self.src_vocab, self.tgt_vocab = source_vocab, target_vocab\n",
    "\n",
    "        # Encode all pairs once for efficiency\n",
    "        self.encoded_pairs: List[Tuple[List[int], List[int]]] = [\n",
    "            (self.src_vocab.encode(src),\n",
    "             self.tgt_vocab.encode(tgt, add_sos=True))\n",
    "            for src, tgt in self.word_pairs\n",
    "        ]\n",
    "        # Also keep just the source sequences for SVD/PPMI embedding\n",
    "        self.encoded_sources: List[List[int]] = [src for src, _ in self.encoded_pairs]\n",
    "\n",
    "        # Padding token id\n",
    "        self.pad_id: int = self.src_vocab.stoi[\"<pad>\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded_pairs)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[List[int], List[int]]:\n",
    "        return self.encoded_pairs[index]\n",
    "\n",
    "\n",
    "def collate_batch(\n",
    "    batch: List[Tuple[List[int], List[int]]],\n",
    "    pad_id: int\n",
    ") -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Pad source and target sequences to uniform length within a batch.\"\"\"\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(s) for s in src_seqs], dtype=torch.long)\n",
    "    tgt_lengths = torch.tensor([len(t) for t in tgt_seqs], dtype=torch.long)\n",
    "\n",
    "    max_src_len = src_lengths.max().item()\n",
    "    max_tgt_len = tgt_lengths.max().item()\n",
    "\n",
    "    padded_sources = torch.full((len(batch), max_src_len), pad_id, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(batch), max_tgt_len), pad_id, dtype=torch.long)\n",
    "\n",
    "    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
    "        padded_sources[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "        padded_targets[i, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "\n",
    "    return padded_sources, src_lengths, padded_targets\n",
    "\n",
    "\n",
    "# ──────────────────── 2.5. Embedding-method modules ────────────────────\n",
    "class OneHotEmbedding(nn.Module):\n",
    "    \"\"\"Convert token ids → explicit one-hot → linear projection to embedding_dim.\"\"\"\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        # Project a one-hot vector of length vocab_size → embedding_dim\n",
    "        self.projection = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # token_ids: (B, T)\n",
    "        one_hot = F.one_hot(token_ids, num_classes=self.vocab_size).float()  # (B,T,V)\n",
    "        # zero out pad positions if desired\n",
    "        one_hot[token_ids == self.padding_idx] = 0.0\n",
    "        # project → (B, T, D)\n",
    "        return self.projection(one_hot)\n",
    "\n",
    "\n",
    "class CharCNNEmbedding(nn.Module):\n",
    "    \"\"\"Convert token ids → one-hot → conv1d over time → (B,T,embedding_dim).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        kernel_size: int = 3,\n",
    "        num_filters: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_filters = num_filters or embedding_dim\n",
    "        # Convolution: in_channels=vocab_size, out_channels=num_filters\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=vocab_size,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            bias=False\n",
    "        )\n",
    "        # Optionally project filters → embedding_dim\n",
    "        if self.num_filters != embedding_dim:\n",
    "            self.projection = nn.Linear(self.num_filters, embedding_dim, bias=False)\n",
    "        else:\n",
    "            self.projection = None\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # token_ids: (B, T)\n",
    "        one_hot = F.one_hot(token_ids, num_classes=self.vocab_size).float()  # (B,T,V)\n",
    "        x = one_hot.permute(0, 2, 1)  # (B, V, T)\n",
    "        x = self.conv1d(x)           # (B, F, T)\n",
    "        x = x.permute(0, 2, 1)       # (B, T, F)\n",
    "        if self.projection:\n",
    "            x = self.projection(x)   # (B, T, D)\n",
    "        return x                     # (B, T, embedding_dim)\n",
    "\n",
    "\n",
    "class SVDPPMIEmbedding(nn.Module):\n",
    "    \"\"\"Build PPMI→SVD char embeddings, then (if needed) project to embedding_dim.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seqs: List[List[int]],\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        window: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 1) Build co-occurrence counts\n",
    "        cooc = np.zeros((vocab_size, vocab_size), dtype=np.float64)\n",
    "        for seq in token_seqs:\n",
    "            for i, u in enumerate(seq):\n",
    "                if u == padding_idx: continue\n",
    "                for j in range(max(0, i - window), min(len(seq), i + window + 1)):\n",
    "                    if i == j: continue\n",
    "                    v = seq[j]\n",
    "                    if v == padding_idx: continue\n",
    "                    cooc[u, v] += 1\n",
    "\n",
    "        # 2) Compute PPMI matrix\n",
    "        total = cooc.sum()\n",
    "        row_sums = cooc.sum(axis=1, keepdims=True)\n",
    "        col_sums = cooc.sum(axis=0, keepdims=True)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            pmi = np.log((cooc * total) / (row_sums * col_sums))\n",
    "        pmi[np.isnan(pmi)] = 0.0\n",
    "        pmi[pmi < 0] = 0.0\n",
    "\n",
    "        # 3) Truncated SVD\n",
    "        U, S, _ = np.linalg.svd(pmi, full_matrices=False)\n",
    "        # D0 is the actual SVD dimension we get (≤ vocab_size)\n",
    "        D0 = min(embedding_dim, U.shape[1])\n",
    "        U = U[:, :D0]            # (vocab_size, D0)\n",
    "        S = S[:D0]               # (D0,)\n",
    "        emb_matrix = U * np.sqrt(S)  # (vocab_size, D0)\n",
    "\n",
    "        # Register static SVD weights\n",
    "        self.register_buffer(\"weight\", torch.from_numpy(emb_matrix).float())\n",
    "\n",
    "        # 4) If the SVD rank D0 is smaller than requested embedding_dim, add a projection\n",
    "        if D0 < embedding_dim:\n",
    "            # Warn the user clearly\n",
    "            warnings.warn(\n",
    "                f\"SVD/PPMI yielded only {D0} dimensions (≤ vocab size), \"\n",
    "                f\"but embedding_size={embedding_dim} was requested. \"\n",
    "                \"Adding a learnable linear projection to expand from \"\n",
    "                f\"{D0} → {embedding_dim} dimensions.\",\n",
    "                UserWarning\n",
    "            )\n",
    "            self.expander = nn.Linear(D0, embedding_dim, bias=False)\n",
    "        else:\n",
    "            self.expander = None\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # Lookup static SVD embeddings → (B, T, D0)\n",
    "        x = F.embedding(token_ids, self.weight, padding_idx=self.weight.new_zeros(1).long())\n",
    "        # If we have an expander, project to the full embedding_dim\n",
    "        if self.expander:\n",
    "            x = self.expander(x)\n",
    "        return x  # (B, T, embedding_dim)\n",
    "\n",
    "\n",
    "# ───────────────────────── 3. Seq2SeqConfig ─────────────────────────\n",
    "@dataclass\n",
    "class Seq2SeqConfig:\n",
    "    \"\"\"Holds hyper-parameters and options for the Seq2Seq model.\"\"\"\n",
    "    # mandatory vocab sizes\n",
    "    source_vocab_size: int\n",
    "    target_vocab_size: int\n",
    "\n",
    "    # embedding / hidden dims\n",
    "    embedding_dim: int = 256\n",
    "    hidden_dim: int = 512\n",
    "\n",
    "    # encoder/decoder depth & type\n",
    "    encoder_layers: int = 2\n",
    "    decoder_layers: int = 2\n",
    "    cell_type: str = \"LSTM\"  # choices: RNN | LSTM | GRU\n",
    "\n",
    "    # dropout for multi-layer RNNs\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # special token indices\n",
    "    pad_index: int = SPECIAL_TOKENS[\"<pad>\"]\n",
    "    sos_index: int = SPECIAL_TOKENS[\"<sos>\"]\n",
    "    eos_index: int = SPECIAL_TOKENS[\"<eos>\"]\n",
    "\n",
    "    # which character embedding method to use\n",
    "    embedding_method: str = \"learned\"  # learned | onehot | char_cnn | svd_ppmi\n",
    "\n",
    "    # only used if embedding_method == \"svd_ppmi\"\n",
    "    svd_sources: Optional[List[List[int]]] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.cell_type in {\"RNN\", \"LSTM\", \"GRU\"}, \"cell_type must be RNN, LSTM or GRU\"\n",
    "        assert self.embedding_method in {\"learned\", \"onehot\", \"char_cnn\", \"svd_ppmi\"}, (\n",
    "            \"embedding_method must be one of learned | onehot | char_cnn | svd_ppmi\"\n",
    "        )\n",
    "        if self.embedding_method == \"svd_ppmi\":\n",
    "            assert self.svd_sources is not None, \"svd_ppmi requires svd_sources\"\n",
    "\n",
    "\n",
    "# map a string to the corresponding nn.RNN module\n",
    "_RNN_MAP: Dict[str, nn.Module] = {\n",
    "    \"RNN\": nn.RNN,\n",
    "    \"LSTM\": nn.LSTM,\n",
    "    \"GRU\": nn.GRU,\n",
    "}\n",
    "\n",
    "# ─────────────────────────── Helper: Align Hidden State ──────────────────────────\n",
    "def _align_hidden_state(hidden_state, target_num_layers: int):\n",
    "    \"\"\"\n",
    "    Adjust the encoder's final hidden state to match the decoder's expected\n",
    "    number of layers. Works for both LSTM (tuple of (h,c)) and GRU/RNN (single tensor).\n",
    "    \n",
    "    Strategies:\n",
    "    - If encoder_layers == decoder_layers: return hidden_state unchanged.\n",
    "    - If encoder_layers  > decoder_layers: take the **last** `target_num_layers` layers.\n",
    "    - If encoder_layers  < decoder_layers: **repeat** the final layer's state\n",
    "      so that the total number of layers equals `target_num_layers`.\n",
    "    \"\"\"\n",
    "    def _repeat_last_layer(tensor, repeat_count: int):\n",
    "        # tensor shape: (enc_layers, batch_size, hidden_dim)\n",
    "        last_layer = tensor[-1:]                             # shape: (1, B, H)\n",
    "        repeated   = last_layer.expand(repeat_count, -1, -1) # shape: (repeat_count, B, H)\n",
    "        return torch.cat([tensor, repeated], dim=0)          # new shape: (enc_layers+repeat_count, B, H)\n",
    "\n",
    "    if isinstance(hidden_state, tuple):\n",
    "        h, c = hidden_state\n",
    "        enc_layers, batch_size, hid_dim = h.shape\n",
    "\n",
    "        if enc_layers == target_num_layers:\n",
    "            return h, c\n",
    "        \n",
    "        # Warn whenever we need to truncate or repeat\n",
    "        warnings.warn(\n",
    "            f\"Encoder has {enc_layers} layers but decoder expects {target_num_layers}. \"\n",
    "            f\"{'Truncating' if enc_layers>target_num_layers else 'Repeating last layer'} hidden state.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "        if enc_layers > target_num_layers:\n",
    "            # keep only the last `target_num_layers` layers\n",
    "            return h[-target_num_layers:], c[-target_num_layers:]\n",
    "        else:\n",
    "            # repeat the final layer's state to pad up to target_num_layers\n",
    "            to_repeat = target_num_layers - enc_layers\n",
    "            return _repeat_last_layer(h, to_repeat), _repeat_last_layer(c, to_repeat)\n",
    "    else:\n",
    "        h = hidden_state\n",
    "        enc_layers, batch_size, hid_dim = h.shape\n",
    "\n",
    "        if enc_layers == target_num_layers:\n",
    "            return h\n",
    "        \n",
    "        # Same warning for single‐tensor hidden states\n",
    "        warnings.warn(\n",
    "            f\"Encoder has {enc_layers} layers but decoder expects {target_num_layers}. \"\n",
    "            f\"{'Truncating' if enc_layers>target_num_layers else 'Repeating last layer'} hidden state.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "        if enc_layers > target_num_layers:\n",
    "            return h[-target_num_layers:]\n",
    "        else:\n",
    "            to_repeat = target_num_layers - enc_layers\n",
    "            return _repeat_last_layer(h, to_repeat)\n",
    "\n",
    "\n",
    "# ───────────────────────── 4. Encoder ─────────────────────────\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder: character embeddings → RNN → hidden state(s).\"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # choose between various embedding modules\n",
    "        if cfg.embedding_method == \"learned\":\n",
    "            self.embedding = nn.Embedding(\n",
    "                cfg.source_vocab_size,\n",
    "                cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "        elif cfg.embedding_method == \"onehot\":\n",
    "            self.embedding = OneHotEmbedding(\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "        elif cfg.embedding_method == \"char_cnn\":\n",
    "            self.embedding = CharCNNEmbedding(\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index,\n",
    "                kernel_size=3\n",
    "            )\n",
    "        elif cfg.embedding_method == \"svd_ppmi\":\n",
    "            self.embedding = SVDPPMIEmbedding(\n",
    "                token_seqs=cfg.svd_sources,\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index,\n",
    "                window=2\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding_method {cfg.embedding_method}\")\n",
    "\n",
    "        # set up the RNN cell\n",
    "        rnn_cls = _RNN_MAP[cfg.cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=cfg.embedding_dim,\n",
    "            hidden_size=cfg.hidden_dim,\n",
    "            num_layers=cfg.encoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=cfg.dropout if cfg.encoder_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_lengths: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src         – LongTensor (B, T_src)\n",
    "            src_lengths – LongTensor (B,) true lengths before padding\n",
    "        Returns:\n",
    "            hidden_state – (h, c) for LSTM or h for GRU/RNN\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(src)  # (B, T_src, D)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, hidden_state = self.rnn(packed)\n",
    "        return hidden_state  # LSTM → (h,c), GRU/RNN → h\n",
    "\n",
    "\n",
    "# ───────────────────────── 5. Decoder ─────────────────────────\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder: one step of embedding → RNN → vocab logits.\"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embedding = nn.Embedding(\n",
    "            cfg.target_vocab_size,\n",
    "            cfg.embedding_dim,\n",
    "            padding_idx=cfg.pad_index\n",
    "        )\n",
    "        rnn_cls = _RNN_MAP[cfg.cell_type]\n",
    "        self.rnn = rnn_cls(\n",
    "            cfg.embedding_dim,\n",
    "            cfg.hidden_dim,\n",
    "            num_layers=cfg.decoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=cfg.dropout if cfg.decoder_layers > 1 else 0.0\n",
    "        )\n",
    "        self.to_vocab_logits = nn.Linear(cfg.hidden_dim, cfg.target_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        current_input: torch.LongTensor,\n",
    "        hidden_state\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor] | torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          current_input – LongTensor (B, 1)\n",
    "          hidden_state  – previous hidden state(s)\n",
    "        Returns:\n",
    "          logits        – (B, 1, V)\n",
    "          hidden_state  – updated hidden state(s)\n",
    "        \"\"\"\n",
    "        emb = self.embedding(current_input)            # (B,1,D)\n",
    "        output, new_hidden = self.rnn(emb, hidden_state)  # (B,1,H)\n",
    "        logits = self.to_vocab_logits(output)          # (B,1,V)\n",
    "        return logits, new_hidden\n",
    "\n",
    "\n",
    "# ───────────────────────── 6. Seq2Seq wrapper ─────────────────────────\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Flexible encoder-decoder wrapper combining Encoder and Decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Seq2SeqConfig):\n",
    "        super().__init__()\n",
    "        self.cfg     = cfg\n",
    "        self.encoder = Encoder(cfg)\n",
    "        self.decoder = Decoder(cfg)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    # Training-time forward (teacher forcing)\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor,\n",
    "        tgt: torch.LongTensor, *,\n",
    "        teacher_forcing_ratio: float = 0.5,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute logits for each timestep in the target sequence using teacher forcing.\n",
    "        Returns logits_all of shape (B, T_tgt, vocab_size).\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        logits_all = torch.zeros(batch_size, tgt_len, self.cfg.target_vocab_size, device=tgt.device)\n",
    "\n",
    "        # 1) Encode source\n",
    "        hidden_state = self.encoder(src, src_lengths)\n",
    "\n",
    "        # 2) Align hidden_state to decoder depth\n",
    "        hidden_state = _align_hidden_state(hidden_state, self.cfg.decoder_layers)\n",
    "\n",
    "        # 3) First decoder input is <sos>\n",
    "        decoder_input = tgt[:, 0].unsqueeze(1)  # (B,1)\n",
    "\n",
    "        # 4) Unroll for each timestep\n",
    "        for t in range(1, tgt_len):\n",
    "            step_logits, hidden_state = self.decoder(decoder_input, hidden_state)\n",
    "            logits_all[:, t] = step_logits.squeeze(1)\n",
    "            # decide next input\n",
    "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                decoder_input = tgt[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = step_logits.argmax(-1)\n",
    "\n",
    "        return logits_all\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    # Greedy decoding\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    def greedy_decode(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor, *,\n",
    "        max_len: int = 50,\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Greedy decoding for inference: always take argmax.\n",
    "        Returns tensor of shape (B, <=max_len).\n",
    "        \"\"\"\n",
    "        B = src.size(0)\n",
    "        hidden_state = self.encoder(src, src_lengths)\n",
    "        hidden_state = _align_hidden_state(hidden_state, self.cfg.decoder_layers)\n",
    "\n",
    "        decoder_input = torch.full(\n",
    "            (B, 1),\n",
    "            self.cfg.sos_index,\n",
    "            dtype=torch.long,\n",
    "            device=src.device\n",
    "        )\n",
    "        generated_ids: List[torch.LongTensor] = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            step_logits, hidden_state = self.decoder(decoder_input, hidden_state)\n",
    "            next_ids = step_logits.argmax(-1)  # (B,1)\n",
    "            generated_ids.append(next_ids)\n",
    "            decoder_input = next_ids\n",
    "\n",
    "        return torch.cat(generated_ids, dim=1)  # (B, seq_len)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    # Beam-search decoding\n",
    "    # ─────────────────────────────────────────────────────────────────────\n",
    "    def beam_search_decode(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor,\n",
    "        *,\n",
    "        beam_size: int = 5,\n",
    "        max_len:   int = 50,\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Beam‐search decoding (batch_size=1 only).\n",
    "        Returns the best generated token sequence *without* the leading <sos>,\n",
    "        as a tensor of shape (1, ≤ max_len).\n",
    "        \"\"\"\n",
    "        # 0) only support batch_size=1 for now\n",
    "        B = src.size(0)\n",
    "        assert B == 1, \"batch_size must be 1 for beam_search_decode\"\n",
    "\n",
    "        # 1) encode + align hidden state\n",
    "        enc_hidden = self.encoder(src, src_lengths)\n",
    "        dec_hidden = _align_hidden_state(enc_hidden, self.cfg.decoder_layers)\n",
    "\n",
    "        # 2) initialize single beam: (tokens, score, hidden_state)\n",
    "        beams = [([self.cfg.sos_index], 0.0, dec_hidden)]\n",
    "        completed = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            # for each live beam, extend by up to beam_size next tokens\n",
    "            for seq, score, hidden in beams:\n",
    "                last = seq[-1]\n",
    "                # if already ended, carry forward to completed and don’t extend\n",
    "                if last == self.cfg.eos_index:\n",
    "                    completed.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                # run one decoder step\n",
    "                inp = torch.tensor([[last]], device=src.device)           # (1,1)\n",
    "                logits, new_hidden = self.decoder(inp, hidden)            # (1,1,V), hidden’\n",
    "                log_probs = torch.log_softmax(logits[0,0], dim=-1)        # (V,)\n",
    "\n",
    "                # grab top‐k continuations\n",
    "                topk_logp, topk_idx = log_probs.topk(beam_size)           # (beam_size,)\n",
    "                for logp, idx in zip(topk_logp.tolist(), topk_idx.tolist()):\n",
    "                    # each candidate gets its own detached copy of new_hidden\n",
    "                    if isinstance(new_hidden, tuple):\n",
    "                        h, c = new_hidden\n",
    "                        h = h.detach().clone()\n",
    "                        c = c.detach().clone()\n",
    "                        nh = (h, c)\n",
    "                    else:\n",
    "                        nh = new_hidden.detach().clone()\n",
    "\n",
    "                    all_candidates.append((seq + [idx], score + logp, nh))\n",
    "\n",
    "            # if no new candidates (all beams ended), stop\n",
    "            if not all_candidates:\n",
    "                break\n",
    "\n",
    "            # keep only the top beam_size beams by score\n",
    "            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "            # if *every* beam has ended with <eos>, we can stop early\n",
    "            if all(seq[-1] == self.cfg.eos_index for seq, _, _ in beams):\n",
    "                completed.extend((seq, sc) for seq, sc, _ in beams)\n",
    "                break\n",
    "\n",
    "        # if nothing ever completed, treat current beams as completed\n",
    "        if not completed:\n",
    "            completed = [(seq, sc) for seq, sc, _ in beams]\n",
    "\n",
    "        # pick the highest‐scoring finished sequence\n",
    "        best_seq, best_score = max(completed, key=lambda x: x[1])\n",
    "\n",
    "        # strip the leading <sos> if present\n",
    "        if best_seq and best_seq[0] == self.cfg.sos_index:\n",
    "            best_seq = best_seq[1:]\n",
    "\n",
    "        # convert to tensor and return (1 x L)\n",
    "        return torch.tensor(best_seq, dtype=torch.long, device=src.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────── 7. Training & evaluation ─────────────────────────\n",
    "def train_epoch(\n",
    "    model: Seq2Seq,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    device: str,\n",
    "    teacher_forcing: float\n",
    ") -> float:\n",
    "    \"\"\"Run one epoch of training; return average token-level cross-entropy loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for src, src_len, tgt in loader:\n",
    "        src, src_len, tgt = src.to(device), src_len.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, src_len, tgt, teacher_forcing_ratio=teacher_forcing)\n",
    "        # shift tgt so we predict t when truth is at t\n",
    "        gold = tgt[:, 1:].contiguous()\n",
    "        preds = logits[:, 1:].contiguous()\n",
    "        loss = loss_fn(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_valid = (gold != loss_fn.ignore_index).sum().item()\n",
    "        total_loss += loss.item() * n_valid\n",
    "        total_tokens += n_valid\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def eval_epoch(\n",
    "    model: Seq2Seq,\n",
    "    loader: DataLoader,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    device: str\n",
    ") -> float:\n",
    "    \"\"\"Run one epoch of evaluation; return average token-level cross-entropy loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_len, tgt in loader:\n",
    "            src, src_len, tgt = src.to(device), src_len.to(device), tgt.to(device)\n",
    "            logits = model(src, src_len, tgt, teacher_forcing_ratio=0.0)\n",
    "            gold = tgt[:, 1:].contiguous()\n",
    "            preds = logits[:, 1:].contiguous()\n",
    "            loss = loss_fn(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "            n_valid = (gold != loss_fn.ignore_index).sum().item()\n",
    "            total_loss += loss.item() * n_valid\n",
    "            total_tokens += n_valid\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "# ───────────────────────── 8. Argument parsing ─────────────────────────\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Train Seq2Seq on the Dakshina Hindi lexicon (character-level transliteration)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--train_tsv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the training lexicon TSV (columns: native_word, romanized_word, count)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--dev_tsv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the development (validation) lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--test_tsv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the test lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Number of full passes over the training data\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of examples per mini-batch\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--embedding_size\",\n",
    "        type=int,\n",
    "        default=256,\n",
    "        help=\"Dimensionality of the character embedding vectors\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--hidden_size\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Size of the hidden states in the encoder and decoder RNNs\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--encoder_layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Number of stacked RNN layers in the encoder\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--decoder_layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Number of stacked RNN layers in the decoder\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--cell\",\n",
    "        choices=[\"RNN\", \"LSTM\", \"GRU\"],\n",
    "        default=\"LSTM\",\n",
    "        help=\"Type of RNN cell to use for both encoder and decoder\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help=\"Learning rate for the Adam optimizer\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--teacher_forcing\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Probability of using teacher forcing at each decoder time step (0.0–1.0)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=DEFAULT_DEVICE,\n",
    "        help=\"Computation device: 'cpu' or 'cuda' (default automatically detects GPU if available)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--embedding_method\",\n",
    "        choices=[\"learned\", \"onehot\", \"char_cnn\", \"svd_ppmi\"],\n",
    "        default=\"learned\",\n",
    "        help=(\n",
    "            \"How to convert characters to vectors: \"\n",
    "            \"'learned' lookup (default), explicit 'onehot'+linear, \"\n",
    "            \"'char_cnn' for a 1D CNN over one-hots, or 'svd_ppmi' for \"\n",
    "            \"static SVD over PPMI co-occurrences\"\n",
    "        )\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--use_attestations\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If set, sample training examples proportional to their annotation count\"\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "# ─────────────────────────── 9. Main driver ───────────────────────────\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # ─── Prepare datasets ───────────────────────────────────────────────\n",
    "    # Train set: build vocabularies and optionally keep counts\n",
    "    train_ds = DakshinaLexicon(\n",
    "        args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=args.use_attestations\n",
    "    )\n",
    "    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n",
    "\n",
    "    # Dev/test: reuse the same vocabs (counts not needed)\n",
    "    dev_ds  = DakshinaLexicon(args.dev_tsv,  src_vocab, tgt_vocab)\n",
    "    test_ds = DakshinaLexicon(args.test_tsv, src_vocab, tgt_vocab)\n",
    "\n",
    "    # Collate function for padding\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "\n",
    "    # Training loader: either shuffle or sample by counts\n",
    "    if args.use_attestations:\n",
    "        assert train_ds.example_counts is not None, \"Attestations requested but counts missing\"\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=train_ds.example_counts,\n",
    "            num_samples=len(train_ds),\n",
    "            replacement=True\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    dev_loader  = DataLoader(dev_ds,  batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # ─── Build model ────────────────────────────────────────────────────\n",
    "    extra_cfg = {}\n",
    "    if args.embedding_method == \"svd_ppmi\":\n",
    "        # supply the token sequences for PPMI/SVD\n",
    "        extra_cfg[\"svd_sources\"] = train_ds.encoded_sources\n",
    "\n",
    "    cfg = Seq2SeqConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=args.embedding_size,\n",
    "        hidden_dim=args.hidden_size,\n",
    "        encoder_layers=args.encoder_layers,\n",
    "        decoder_layers=args.decoder_layers,\n",
    "        cell_type=args.cell,\n",
    "        dropout=0.1,\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=args.embedding_method,\n",
    "        **extra_cfg\n",
    "    )\n",
    "    model = Seq2Seq(cfg).to(args.device)\n",
    "\n",
    "    # Optimizer & loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    # ─── Training loop ───────────────────────────────────────────────────\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, args.device, args.teacher_forcing)\n",
    "        dev_loss   = eval_epoch(model, dev_loader, loss_fn, args.device)\n",
    "        print(f\"Epoch {epoch:02d} | train ppl={math.exp(train_loss):.2f} | dev ppl={math.exp(dev_loss):.2f}\")\n",
    "\n",
    "    # ─── Final evaluation on test ────────────────────────────────────────\n",
    "    test_loss = eval_epoch(model, test_loader, loss_fn, args.device)\n",
    "    print(f\"Test perplexity: {math.exp(test_loss):.2f}\")\n",
    "\n",
    "    # ─── Qualitative samples ─────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    print(\"Sample dev-set transliterations:\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(5):\n",
    "            src_ids, tgt_ids = dev_ds[i]\n",
    "            src_len = len(src_ids)\n",
    "            src_tensor = torch.tensor([src_ids], device=args.device)\n",
    "            len_tensor = torch.tensor([src_len], device=args.device)\n",
    "\n",
    "            pred_ids = model.greedy_decode(src_tensor, len_tensor, max_len=30)[0].tolist()\n",
    "            romanized = src_vocab.decode(src_ids)\n",
    "            gold       = tgt_vocab.decode(tgt_ids[1:])  # skip <sos>\n",
    "            pred_str   = tgt_vocab.decode(pred_ids[1:])  # skip <sos>\n",
    "            print(f\"{romanized:15} → {pred_str:15} (gold: {gold})\")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"train_dakshina_seq2seq.py\",\n",
    "    \"--train_tsv\", \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\",   \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\",  \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--embedding_size\", \"128\",\n",
    "    \"--hidden_size\",    \"256\",\n",
    "    \"--encoder_layers\", \"1\",\n",
    "    \"--decoder_layers\", \"1\",\n",
    "    \"--cell\", \"GRU\",\n",
    "    \"--epochs\", \"3\",\n",
    "    \"--embedding_method\", \"learned\",\n",
    "    \"--use_attestations\",\n",
    "]\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c9659",
   "metadata": {},
   "source": [
    "# Q2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q2: W&B sweep driver for character-level Hindi transliteration\n",
    "────────────────────────────────────────────────────────────────\n",
    "Re-uses the full Seq2Seq implementation from solution_1.py (Q1).\n",
    "A YAML file under ./configs/ specifies the sweep search space.\n",
    "\n",
    "Example usage\n",
    "~~~~~~~~~~~~~\n",
    "# 1) Create the sweep and directly launch the agent:\n",
    "python solution_2.py \\\n",
    "    --mode sweep \\\n",
    "    --sweep_config sweep_config.yaml \\\n",
    "    --wandb_project transliteration \\\n",
    "    --wandb_run_tag baseline \\\n",
    "    --gpu_ids 0 1 \\\n",
    "    --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "    --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "    --test_tsv  ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "    --sweep_count 30\n",
    "\n",
    "# 2) For a single debug run:\n",
    "python solution_2.py \\\n",
    "    --mode single \\\n",
    "    --wandb_project transliteration \\\n",
    "    --wandb_run_tag baseline \\\n",
    "    --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "    --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "    --test_tsv  ./lexicons/hi.translit.sampled.test.tsv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ───────────────────────── Imports ─────────────────────────\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "# ────────────── Helpers ──────────────\n",
    "\n",
    "def get_configs(project_root: str | Path, config_filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load a YAML sweep configuration from ./configs/ relative to project_root.\n",
    "    \"\"\"\n",
    "    cfg_path = Path(project_root) / \"configs\" / config_filename\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        config: Dict[str, Any] = yaml.safe_load(handle)\n",
    "    return config\n",
    "\n",
    "\n",
    "def compute_sequence_accuracy(\n",
    "    model: Seq2Seq,\n",
    "    dataset: DakshinaLexicon,\n",
    "    device: str,\n",
    "    beam_size: int = 1\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute exact-match accuracy over a dataset using greedy or beam decoding.\n",
    "\n",
    "    Args:\n",
    "        model      – trained Seq2Seq module (unwrapped if DataParallel)\n",
    "        dataset    – DakshinaLexicon providing (src_ids, tgt_ids)\n",
    "        device     – 'cpu' or 'cuda'\n",
    "        beam_size  – if >1, use beam_search_decode; else greedy_decode\n",
    "    Returns:\n",
    "        accuracy   – fraction of examples where pred == gold\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src_ids, tgt_ids in dataset:\n",
    "            # prepare input tensors\n",
    "            src_tensor = torch.tensor([src_ids], device=device)\n",
    "            src_len_tensor = torch.tensor([len(src_ids)], device=device)\n",
    "\n",
    "\n",
    "            pred_ids = model.beam_search_decode(\n",
    "                src_tensor, src_len_tensor,\n",
    "                beam_size=beam_size, max_len=len(tgt_ids)\n",
    "            )[0]\n",
    "\n",
    "            # convert to strings (skip leading <sos>)\n",
    "            gold_str = dataset.tgt_vocab.decode(tgt_ids[1:])\n",
    "\n",
    "            # convert to strings, *dropping* the leading <sos> token\n",
    "            pred_ids_list = pred_ids.tolist()\n",
    "            if pred_ids_list and pred_ids_list[0] == dataset.tgt_vocab.stoi[\"<sos>\"]:\n",
    "                pred_ids_list = pred_ids_list[1:]\n",
    "            pred_str = dataset.tgt_vocab.decode(pred_ids_list)\n",
    "            correct += int(pred_str == gold_str)\n",
    "            total += 1\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "# ────────────── 2. A single training run (used by sweep) ──────────────\n",
    "def run_single_training(sweep_config: Dict[str, Any], static_args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Train + evaluate once using the hyper-parameters in sweep_config and\n",
    "    the static file paths / tag supplied via CLI.\n",
    "    \"\"\"\n",
    "    # ─── Honour multi-GPU pinning via CUDA_VISIBLE_DEVICES ─────────────────\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(g) for g in static_args.gpu_ids)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ─── Warn if encoder/decoder depths differ ─────────────────────────────\n",
    "    if sweep_config[\"encoder_layers\"] != sweep_config[\"decoder_layers\"]:\n",
    "        warnings.warn(\n",
    "            f\"Encoder layers ({sweep_config['encoder_layers']}) != \"\n",
    "            f\"decoder layers ({sweep_config['decoder_layers']}), \"\n",
    "            \"hidden states will be aligned automatically.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "    # ─── Data loading ────────────────────────────────────────────────\n",
    "    train_dataset = DakshinaLexicon(\n",
    "        static_args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=sweep_config.get(\"use_attestations\", False),\n",
    "    )\n",
    "    src_vocab: CharVocabulary = train_dataset.src_vocab\n",
    "    tgt_vocab: CharVocabulary = train_dataset.tgt_vocab\n",
    "\n",
    "    dev_dataset  = DakshinaLexicon(static_args.dev_tsv,  src_vocab, tgt_vocab)\n",
    "    test_dataset = DakshinaLexicon(static_args.test_tsv, src_vocab, tgt_vocab)\n",
    "\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=sweep_config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    dev_loader  = torch.utils.data.DataLoader(\n",
    "        dev_dataset,\n",
    "        batch_size=sweep_config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=sweep_config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # ─── Build the Seq2Seq model ─────────────────────────────────────────\n",
    "    extra_cfg: Dict[str, Any] = {}\n",
    "    if sweep_config[\"embedding_method\"] == \"svd_ppmi\":\n",
    "        extra_cfg[\"svd_sources\"] = train_dataset.encoded_sources\n",
    "\n",
    "    model_cfg = Seq2SeqConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=sweep_config[\"embedding_size\"],\n",
    "        hidden_dim=sweep_config[\"hidden_size\"],\n",
    "        encoder_layers=sweep_config[\"encoder_layers\"],\n",
    "        decoder_layers=sweep_config[\"decoder_layers\"],\n",
    "        cell_type=sweep_config[\"cell\"],\n",
    "        dropout=sweep_config[\"dropout\"],\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=sweep_config[\"embedding_method\"],\n",
    "        **extra_cfg,\n",
    "    )\n",
    "    model = Seq2Seq(model_cfg)\n",
    "\n",
    "    # ─── Wrap in DataParallel if multiple GPUs specified ─────────────────\n",
    "    if device == \"cuda\" and len(static_args.gpu_ids) > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=sweep_config[\"learning_rate\"])\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=model_cfg.pad_index)\n",
    "\n",
    "    # ─── Unique run name ─────────────────────────────\n",
    "    run_name = (\n",
    "        f\"emb_method:{sweep_config['embedding_method']}|emb_size:{sweep_config['embedding_size']}|\"\n",
    "        f\"cell:{sweep_config['cell']}|hid_size:{sweep_config['hidden_size']}|\"\n",
    "        f\"enc:{sweep_config['encoder_layers']}|dec:{sweep_config['decoder_layers']}|\"\n",
    "        f\"dr:{sweep_config['dropout']}|lr:{sweep_config['learning_rate']}|\"\n",
    "        f\"bs:{sweep_config['batch_size']}|tf:{sweep_config['teacher_forcing']}|\"\n",
    "        f\"ep:{sweep_config['epochs']}|beam:{sweep_config['beam_size']}|\"\n",
    "        f\"attest:{sweep_config['use_attestations']}\"\n",
    "    )\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.tags = [static_args.wandb_run_tag]\n",
    "\n",
    "    # ─── Training loop ───────────────────────────────────────────────\n",
    "    for epoch in range(1, sweep_config[\"epochs\"] + 1):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss_fn, device, sweep_config[\"teacher_forcing\"]\n",
    "        )\n",
    "        dev_loss = eval_epoch(model, dev_loader, loss_fn, device)\n",
    "\n",
    "        # Log both raw loss and perplexity\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_perplexity\": math.exp(train_loss),\n",
    "            \"dev_loss\":   dev_loss,\n",
    "            \"dev_perplexity\": math.exp(dev_loss),\n",
    "        })\n",
    "\n",
    "\n",
    "    # ─── Compute and log sequence-level accuracies ─────────────────────\n",
    "    # Unwrap from DataParallel if needed for decoding\n",
    "    inference_model = model.module if hasattr(model, \"module\") else model\n",
    "    beam = sweep_config.get(\"beam_size\", 1)\n",
    "\n",
    "    # training_accuracy = compute_sequence_accuracy(inference_model, train_dataset, device, beam_size=1)\n",
    "    dev_accuracy      = compute_sequence_accuracy(inference_model, dev_dataset,   device, beam_size=beam)\n",
    "    # test_accuracy     = compute_sequence_accuracy(inference_model, test_dataset,  device, beam_size=beam)\n",
    "\n",
    "    wandb.log({\n",
    "        \"dev_accuracy\":   dev_accuracy,\n",
    "    })\n",
    "\n",
    "    # ─── Qualitative beam-search samples ─────────────────────────────\n",
    "    print(f\"\\nSample dev-set translations (beam_size={beam}):\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(5):\n",
    "            src_ids, tgt_ids = dev_dataset[i]  # dev_dataset returns (src, tgt)\n",
    "            src_len = len(src_ids)\n",
    "            src_tensor = torch.tensor([src_ids], device=device)\n",
    "            len_tensor = torch.tensor([src_len], device=device)\n",
    "\n",
    "            # always use beam-search (beam_size may be 1)\n",
    "            pred_ids = inference_model.beam_search_decode(\n",
    "                src_tensor,\n",
    "                len_tensor,\n",
    "                beam_size=beam,\n",
    "                max_len=30\n",
    "            )[0].tolist()\n",
    "\n",
    "            romanized = src_vocab.decode(src_ids)\n",
    "            gold       = tgt_vocab.decode(tgt_ids[1:])  # skip <sos>\n",
    "            prediction = tgt_vocab.decode(pred_ids[1:])  # skip <sos>\n",
    "            print(f\"{romanized:15} → {prediction:15} (gold: {gold})\")\n",
    "\n",
    "\n",
    "# ────────────── 3. Sweep & CLI plumbing ──────────────\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Launch or run a W&B sweep for Q2.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mode\", choices=[\"sweep\", \"single\"], required=True,\n",
    "        help=\"'sweep' to create & launch the sweep; 'single' for debug run\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sweep_config\", type=str, default=\"sweep_config.yaml\",\n",
    "        help=\"YAML filename under ./configs/ defining the sweep space\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_project\", type=str, required=True, help=\"W&B project name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb_run_tag\", type=str, default=\"baseline\",\n",
    "        help=\"Static tag added to every W&B run\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gpu_ids\", type=int, nargs=\"+\", default=[0],\n",
    "        help=\"CUDA device IDs to use (e.g. --gpu_ids 0 1 for two GPUs)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_tsv\", type=str, required=True, help=\"Path to train TSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dev_tsv\",   type=str, required=True, help=\"Path to dev TSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_tsv\",  type=str, required=True, help=\"Path to test TSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sweep_count\", type=int, default=30,\n",
    "        help=\"Number of sweep runs to launch if mode==sweep\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load sweep_config YAML\n",
    "    project_root = Path.cwd().resolve().parent\n",
    "    sweep_yaml   = get_configs(project_root, args.sweep_config)\n",
    "\n",
    "    if args.mode == \"sweep\":\n",
    "        # ─── Prepare sweep definition ─────────────────────────────────\n",
    "        sweep_yaml.setdefault(\"method\", \"bayes\")\n",
    "        sweep_yaml.setdefault(\n",
    "            \"metric\", {\"name\": \"dev_perplexity\", \"goal\": \"minimize\"}\n",
    "        )\n",
    "        sweep_yaml.setdefault(\"parameters\", {})\n",
    "        sweep_yaml[\"run_cap\"]    = args.sweep_count\n",
    "\n",
    "        # ─── Register the sweep with W&B ───────────────────────────────\n",
    "        sweep_id = wandb.sweep(\n",
    "            sweep=sweep_yaml,\n",
    "            project=args.wandb_project\n",
    "        )\n",
    "        print(f\"Registered sweep with id: {sweep_id}\")\n",
    "\n",
    "        # ─── Define the function the agent will call for each trial ────\n",
    "        def sweep_train():\n",
    "            # start a new W&B run for this trial\n",
    "            run = wandb.init(\n",
    "                project=args.wandb_project\n",
    "            )\n",
    "            # now wandb.config is initialized, so we can read it\n",
    "            run_single_training(dict(run.config), args)\n",
    "            run.finish()\n",
    "\n",
    "        # ─── Launch W&B agent in-process ───────────────────────────────\n",
    "        print(f\"Launching W&B agent for sweep {sweep_id}, count={args.sweep_count}\")\n",
    "        wandb.agent(\n",
    "            sweep_id,\n",
    "            function=sweep_train,\n",
    "            count=args.sweep_count\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # ─── Single run for debugging ───────────────────────────────────\n",
    "        with wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            config=sweep_yaml.get(\"parameters\", {}),\n",
    "        ) as run:\n",
    "            run.config.update(\n",
    "                {\n",
    "                    \"epochs\": 3,\n",
    "                    \"batch_size\": 64,\n",
    "                    \"embedding_size\": 128,\n",
    "                    \"hidden_size\": 256,\n",
    "                    \"encoder_layers\": 2,\n",
    "                    \"decoder_layers\": 2,\n",
    "                    \"cell\": \"LSTM\",\n",
    "                    \"dropout\": 0.1,\n",
    "                    \"learning_rate\": 1e-3,\n",
    "                    \"teacher_forcing\": 0.5,\n",
    "                    \"embedding_method\": \"learned\",\n",
    "                    \"use_attestations\": False,\n",
    "                    \"beam_size\": 1,\n",
    "                },\n",
    "                allow_val_change=True,\n",
    "            )\n",
    "            run_single_training(dict(run.config), args)\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# simulate exactly what you'd type on the command line:\n",
    "sys.argv = [\n",
    "    \"solution_2.py\",\n",
    "    \"--mode\", \"sweep\",\n",
    "    \"--sweep_config\", \"sweep_config.yaml\",\n",
    "    \"--wandb_project\", \"transliteration\",\n",
    "    \"--wandb_run_tag\", \"solution_2\",\n",
    "    \"--gpu_ids\", \"0\",\n",
    "    \"--train_tsv\", \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\", \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\", \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--sweep_count\", \"50\",\n",
    "]\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8734a",
   "metadata": {},
   "source": [
    "### Q2 Solution - Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "solution_2b.py: Tune beam size for your pretrained Hindi transliteration model.\n",
    "\n",
    "This script:\n",
    "  1. Loads the Hindi Dakshina train/dev/test splits.\n",
    "  2. Trains a Seq2Seq model with your best hyper-parameters.\n",
    "  3. Runs beam-search decoding on the dev set for various beam sizes.\n",
    "  4. Reports exact‐match accuracy for each beam size.\n",
    "\n",
    "Usage example:\n",
    "    python solution_2b.py \\\n",
    "        --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "        --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "        --test_tsv  ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "        --gpu_ids   3\n",
    "\n",
    "Arguments:\n",
    "  --train_tsv   Path to the Hindi training lexicon TSV\n",
    "  --dev_tsv     Path to the Hindi development lexicon TSV\n",
    "  --test_tsv    Path to the Hindi test lexicon TSV\n",
    "  --gpu_ids     CUDA device IDs to use (e.g. --gpu_ids 0 1)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ─── best hyper-parameters found ─────────────────────────────\n",
    "best = {\n",
    "    \"batch_size\":        32,\n",
    "    \"cell\":             \"LSTM\",\n",
    "    \"decoder_layers\":     2,\n",
    "    \"dropout\":          0.2,\n",
    "    \"embedding_method\":\"learned\",\n",
    "    \"embedding_size\":    64,\n",
    "    \"encoder_layers\":     2,\n",
    "    \"epochs\":            10,\n",
    "    \"hidden_size\":       256,\n",
    "    \"learning_rate\":   0.0006772525707570603,\n",
    "    \"teacher_forcing\":   0.7,\n",
    "    \"use_attestations\":  False,\n",
    "}\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Train with fixed hyperparams and tune beam size\")\n",
    "    p.add_argument(\"--train_tsv\",  type=str, required=True, help=\"Path to Hindi train lexicon TSV\")\n",
    "    p.add_argument(\"--dev_tsv\",    type=str, required=True, help=\"Path to Hindi dev lexicon TSV\")\n",
    "    p.add_argument(\"--test_tsv\",   type=str, required=True, help=\"Path to Hindi test lexicon TSV\")\n",
    "    p.add_argument(\n",
    "        \"--gpu_ids\", type=int, nargs=\"+\", default=[0],\n",
    "        help=\"CUDA device IDs to use (e.g. 0 1).\"\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "def build_data_loaders(\n",
    "    train_path: str,\n",
    "    dev_path:   str,\n",
    "    test_path:  str,\n",
    "    batch_size: int,\n",
    "    use_attest: bool\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, CharVocabulary, CharVocabulary]:\n",
    "    \"\"\"\n",
    "    Builds train/dev/test datasets and loaders, re-using vocab built on train.\n",
    "    If use_attest=True, uses a WeightedRandomSampler on train counts.\n",
    "    \"\"\"\n",
    "    # build train ds + vocabs\n",
    "    train_ds = DakshinaLexicon(train_path, build_vocabs=True, use_attestations=use_attest)\n",
    "    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n",
    "\n",
    "    # dev/test reuse same vocabs\n",
    "    dev_ds  = DakshinaLexicon(dev_path,  src_vocab, tgt_vocab)\n",
    "    test_ds = DakshinaLexicon(test_path, src_vocab, tgt_vocab)\n",
    "\n",
    "    # collate_fn for padding\n",
    "    pad_id = src_vocab.stoi[\"<pad>\"]\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=pad_id)\n",
    "\n",
    "    # train loader: optionally weighted by attestations\n",
    "    if use_attest:\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=train_ds.example_counts,\n",
    "            num_samples=len(train_ds),\n",
    "            replacement=True\n",
    "        )\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
    "                                  sampler=sampler, collate_fn=collate_fn)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
    "                                  shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # dev/test\n",
    "    dev_loader  = DataLoader(dev_ds,  batch_size=batch_size,\n",
    "                              shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size,\n",
    "                              shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, dev_loader, test_loader, src_vocab, tgt_vocab\n",
    "\n",
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader:   DataLoader,\n",
    "    cfg:          Seq2SeqConfig,\n",
    "    device:       torch.device,\n",
    "    epochs:       int,\n",
    "    lr:           float,\n",
    "    teacher_force:    float\n",
    ") -> Seq2Seq:\n",
    "    \"\"\"\n",
    "    Trains the Seq2Seq model for `epochs` epochs.\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "    model = Seq2Seq(cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn   = torch.nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader,\n",
    "                                 optimizer, loss_fn, device, teacher_force)\n",
    "        dev_loss   = eval_epoch(model, dev_loader, loss_fn, device)\n",
    "        print(f\"Epoch {epoch:02d} | \"\n",
    "              f\"train ppl={math.exp(train_loss):.2f} | \"\n",
    "              f\"dev   ppl={math.exp(dev_loss):.2f}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_beam_exact_match(\n",
    "    model:       Seq2Seq,\n",
    "    dataset:     DakshinaLexicon,\n",
    "    src_vocab:   CharVocabulary,\n",
    "    tgt_vocab:   CharVocabulary,\n",
    "    beam_size:   int,\n",
    "    device:      torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Runs beam_search_decode on *each* example in `dataset` (batch_size=1),\n",
    "    computes exact‐match rate vs. gold target.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # unwrap DataParallel if needed\n",
    "    inference_model = getattr(model, \"module\", model)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src_ids, tgt_ids in tqdm(dataset, desc=f\"beam={beam_size}\", leave=False):\n",
    "            # prepare tensors\n",
    "            src_len = len(src_ids)\n",
    "            src_tensor = torch.tensor([src_ids], device=device)\n",
    "            len_tensor = torch.tensor([src_len], device=device)\n",
    "\n",
    "            # beam search decode\n",
    "            pred_ids = inference_model.beam_search_decode(\n",
    "                src_tensor, len_tensor,\n",
    "                beam_size=beam_size,\n",
    "                max_len= max(len(src_ids)*2, 30)\n",
    "            )[0].tolist()\n",
    "\n",
    "            # decode strings\n",
    "            pred_str = tgt_vocab.decode(pred_ids)\n",
    "            gold_str = tgt_vocab.decode(tgt_ids[1:])  # skip <sos>\n",
    "\n",
    "            if pred_str == gold_str:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    # pin visible GPUs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, args.gpu_ids))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ─── build data loaders ────────────────────────────────────────────\n",
    "    train_loader, dev_loader, test_loader, src_vocab, tgt_vocab = build_data_loaders(\n",
    "        train_path= args.train_tsv,\n",
    "        dev_path=   args.dev_tsv,\n",
    "        test_path=  args.test_tsv,\n",
    "        batch_size= best[\"batch_size\"],\n",
    "        use_attest= best[\"use_attestations\"],\n",
    "    )\n",
    "\n",
    "    # ─── construct model config ────────────────────────────────────────\n",
    "    model_cfg = Seq2SeqConfig(\n",
    "        source_vocab_size= src_vocab.size,\n",
    "        target_vocab_size= tgt_vocab.size,\n",
    "        embedding_dim=     best[\"embedding_size\"],\n",
    "        hidden_dim=        best[\"hidden_size\"],\n",
    "        encoder_layers=    best[\"encoder_layers\"],\n",
    "        decoder_layers=    best[\"decoder_layers\"],\n",
    "        cell_type=         best[\"cell\"],\n",
    "        dropout=           best[\"dropout\"],\n",
    "        pad_index=         src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=         tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=         tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=  best[\"embedding_method\"],\n",
    "        # no SVD here since embedding_method=\"onehot\"\n",
    "    )\n",
    "\n",
    "    # ─── train the model ────────────────────────────────────────────────\n",
    "    model = train_model(\n",
    "        train_loader, dev_loader,\n",
    "        cfg=model_cfg,\n",
    "        device=device,\n",
    "        epochs=best[\"epochs\"],\n",
    "        lr=best[\"learning_rate\"],\n",
    "        teacher_force=best[\"teacher_forcing\"],\n",
    "    )\n",
    "\n",
    "    # ─── final test perplexity ───────────────────────────────────────────\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=model_cfg.pad_index)\n",
    "    test_loss = eval_epoch(model, test_loader, loss_fn, device)\n",
    "    print(f\"\\nTest perplexity: {math.exp(test_loss):.2f}\\n\")\n",
    "\n",
    "    # ─── beam‐size sweep on dev set ──────────────────────────────────────\n",
    "    print(\"Tuning beam size on dev set (exact‐match rate):\")\n",
    "    for beam in [1, 2, 3, 5, 8, 10]:\n",
    "        acc = evaluate_beam_exact_match(\n",
    "            model, dev_loader.dataset, src_vocab, tgt_vocab,\n",
    "            beam_size=beam, device=device\n",
    "        )\n",
    "        print(f\"  beam_size={beam:2d} → dev accuracy = {acc * 100:5.2f}%\")\n",
    "\n",
    "import sys\n",
    "\n",
    "# simulate command‐line\n",
    "sys.argv = [\n",
    "    \"solution_2b.py\",\n",
    "    \"--train_tsv\", \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\",   \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\",  \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--gpu_ids\",   \"3\",\n",
    "]\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac215c7",
   "metadata": {},
   "source": [
    "# Q4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07301cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q4: Train (if needed) & evaluate best model on Dakshina transliteration test set\n",
    "\n",
    "Usage example:\n",
    "    # 1) Train+evaluate (if no checkpoint exists) or just evaluate:\n",
    "    python solution_4.py \\\n",
    "      --train_tsv ./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv \\\n",
    "      --dev_tsv   ./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv \\\n",
    "      --test_tsv  ./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv \\\n",
    "      --checkpoint ./checkpoints/best_seq2seq.pt \\\n",
    "      --output_dir predictions_vanilla \\\n",
    "      --gpu_ids 0 1 \\\n",
    "      --wandb_project transliteration \\\n",
    "      --wandb_run_name solution_4_run \\\n",
    "      --wandb_run_tag solution_4\n",
    "\n",
    "    # 2) If you don't want WandB logging, simply omit --wandb_project / --wandb_run_tag\n",
    "Outputs:\n",
    "  - Trains with early stopping (patience=3) if checkpoint not found.\n",
    "  - Saves checkpoint to `--checkpoint`.\n",
    "  - Prints exact‐match accuracy on test set.\n",
    "  - Saves all predictions in both TSV and CSV under `--output_dir`.\n",
    "  - Builds a colored table of 10 samples, logs it to WandB, and prints a markdown table.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import wandb\n",
    "from wcwidth import wcswidth\n",
    "\n",
    "# ────────────────────────────────\n",
    "# Download & register Devanagari font\n",
    "# ────────────────────────────────\n",
    "font_dir = Path.cwd().parent / \"fonts\"\n",
    "font_dir.mkdir(exist_ok=True)\n",
    "font_path = font_dir / \"Hind-Regular.ttf\"\n",
    "\n",
    "if not font_path.exists():\n",
    "    print(\"Font file not found. Please ensure 'Hind-Regular.ttf' is in the 'fonts' directory which\"\n",
    "    \"can be downloaded from Google Fonts. https://www.cufonfonts.com/font/noto-sans-devanagari\")\n",
    "else:\n",
    "    fm.fontManager.addfont(str(font_path))\n",
    "    plt.rcParams[\"font.family\"] = \"Hind\"\n",
    "    plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "    print(\"Font loaded and matplotlib configured.\")\n",
    "\n",
    "# ────────────────────────────────\n",
    "# GLOBAL: best hyperparameters\n",
    "# ────────────────────────────────\n",
    "best = {\n",
    "    \"batch_size\":       32,\n",
    "    \"beam_size\":        5,\n",
    "    \"cell\":             \"LSTM\",\n",
    "    \"decoder_layers\":   2,\n",
    "    \"dropout\":          0.2,\n",
    "    \"embedding_method\": \"learned\",\n",
    "    \"embedding_size\":   64,\n",
    "    \"encoder_layers\":   2,\n",
    "    \"epochs\":           25,\n",
    "    \"hidden_size\":      256,\n",
    "    \"learning_rate\":    0.0006772525707570603,\n",
    "    \"teacher_forcing\":  0.7,\n",
    "    \"use_attestations\": False,\n",
    "    # early stopping patience\n",
    "    \"patience\":         3,\n",
    "}\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Q4: Train (if needed) and apply best Seq2Seq model to test set\"\n",
    "    )\n",
    "    parser.add_argument(\"--train_tsv\",   type=str, required=True,\n",
    "                        help=\"Path to training lexicon TSV (for vocabulary)\")\n",
    "    parser.add_argument(\"--dev_tsv\",     type=str, required=True,\n",
    "                        help=\"Path to development lexicon TSV (for early stopping)\")\n",
    "    parser.add_argument(\"--test_tsv\",    type=str, required=True,\n",
    "                        help=\"Path to test lexicon TSV\")\n",
    "    parser.add_argument(\"--checkpoint\",  type=str, required=True,\n",
    "                        help=\"Path to save/load model checkpoint (.pt file)\")\n",
    "    parser.add_argument(\"--output_dir\",  type=str, default=\"predictions_vanilla\",\n",
    "                        help=\"Directory to write predictions.tsv and predictions.csv\")\n",
    "    parser.add_argument(\"--gpu_ids\",     type=int, nargs=\"+\", default=[0],\n",
    "                        help=\"CUDA device IDs to use (e.g. 0 1).\")\n",
    "    parser.add_argument(\"--wandb_project\",  type=str, default=None,\n",
    "                        help=\"W&B project name (omit to disable WandB)\")\n",
    "    parser.add_argument(\"--wandb_run_name\", type=str, default=None,\n",
    "                        help=\"W&B run name (omit to disable WandB)\")\n",
    "    parser.add_argument(\"--wandb_run_tag\",  type=str, default=\"baseline\",\n",
    "                        help=\"Static tag added to every W&B run\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ─── 0. Pin GPUs ───────────────────────────────────────────────\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(g) for g in args.gpu_ids)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ─── Initialize WandB if requested ─────────────────────────────\n",
    "    use_wandb = args.wandb_project is not None\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            name=args.wandb_run_name,\n",
    "            tags=[args.wandb_run_tag],\n",
    "            config=best\n",
    "        )\n",
    "\n",
    "    # ensure output dirs exist\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(args.checkpoint).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ─── 1. Build vocabulary from train set ───────────────────────────\n",
    "    train_ds = DakshinaLexicon(\n",
    "        args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=best[\"use_attestations\"]\n",
    "    )\n",
    "    src_vocab = train_ds.src_vocab\n",
    "    tgt_vocab = train_ds.tgt_vocab\n",
    "\n",
    "    # ─── 2. Prepare dev/test loaders ─────────────────────────────────\n",
    "    dev_ds  = DakshinaLexicon(args.dev_tsv,  src_vocab, tgt_vocab)\n",
    "    test_ds = DakshinaLexicon(args.test_tsv, src_vocab, tgt_vocab)\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "\n",
    "    dev_loader = DataLoader(dev_ds,  batch_size=best[\"batch_size\"],\n",
    "                            shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=best[\"batch_size\"],\n",
    "                             shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # ─── 3. Build model & optimizer ───────────────────────────────────\n",
    "    cfg = Seq2SeqConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=best[\"embedding_size\"],\n",
    "        hidden_dim=best[\"hidden_size\"],\n",
    "        encoder_layers=best[\"encoder_layers\"],\n",
    "        decoder_layers=best[\"decoder_layers\"],\n",
    "        cell_type=best[\"cell\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=best[\"embedding_method\"],\n",
    "    )\n",
    "    model     = Seq2Seq(cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best[\"learning_rate\"])\n",
    "    loss_fn   = torch.nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    # ─── 4. Train w/ early stopping if no checkpoint ─────────────────\n",
    "    if not os.path.exists(args.checkpoint):\n",
    "        print(\"No checkpoint found; starting training with early stopping...\")\n",
    "        best_dev, no_improve = float(\"inf\"), 0\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=best[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        for epoch in range(1, best[\"epochs\"] + 1):\n",
    "            train_loss = train_epoch(\n",
    "                model, train_loader, optimizer, loss_fn,\n",
    "                device, best[\"teacher_forcing\"]\n",
    "            )\n",
    "            dev_loss = eval_epoch(model, dev_loader, loss_fn, device)\n",
    "            train_ppl = math.exp(train_loss)\n",
    "            dev_ppl   = math.exp(dev_loss)\n",
    "            print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} ppl={train_ppl:.2f} \"\n",
    "                  f\"| dev_loss={dev_loss:.4f} ppl={dev_ppl:.2f}\")\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    \"Q4_epoch\": epoch,\n",
    "                    \"Q4_train_loss\": train_loss,\n",
    "                    \"Q4_train_ppl\": train_ppl,\n",
    "                    \"Q4_dev_loss\": dev_loss,\n",
    "                    \"Q4_dev_ppl\": dev_ppl,\n",
    "                })\n",
    "            # early stopping\n",
    "            if dev_loss < best_dev:\n",
    "                best_dev, no_improve = dev_loss, 0\n",
    "                torch.save({\"model_state_dict\": model.state_dict()}, args.checkpoint)\n",
    "                print(\"  ↳ dev improved; checkpoint saved.\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                print(f\"  ↳ no improvement for {no_improve} epoch(s)\")\n",
    "                if no_improve >= best[\"patience\"]:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "        print(\"Training finished.\\n\")\n",
    "    else:\n",
    "        print(f\"Found checkpoint at {args.checkpoint}; skipping training.\\n\")\n",
    "\n",
    "    # ─── 5. Load checkpoint & evaluate on test set ───────────────────\n",
    "    ckpt = torch.load(args.checkpoint, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device).eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    all_preds = []  # list of (source, gold, prediction)\n",
    "    with torch.no_grad():\n",
    "        for src_batch, src_lens, tgt_batch in test_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            src_lens  = src_lens.to(device)\n",
    "            for i in range(src_batch.size(0)):\n",
    "                total += 1\n",
    "                single_src   = src_batch[i].unsqueeze(0)\n",
    "                single_len   = src_lens[i].unsqueeze(0)\n",
    "                gold_ids     = tgt_batch[i].tolist()[1:]  # skip <sos>\n",
    "                pred_ids     = model.beam_search_decode(\n",
    "                                    single_src, single_len,\n",
    "                                    beam_size=best[\"beam_size\"], max_len=50\n",
    "                               )[0].tolist()\n",
    "                src_str      = src_vocab.decode(single_src[0].tolist())\n",
    "                gold_str     = tgt_vocab.decode(gold_ids)\n",
    "                pred_str     = tgt_vocab.decode(pred_ids)\n",
    "                if pred_str == gold_str:\n",
    "                    correct += 1\n",
    "                all_preds.append((src_str, gold_str, pred_str))\n",
    "\n",
    "    test_loss = eval_epoch(model, test_loader, loss_fn, device)\n",
    "    test_ppl  = math.exp(test_loss)\n",
    "    accuracy  = correct / total * 100\n",
    "    print(f\"\\nTest loss={test_loss:.4f} ppl={test_ppl:.2f} \"\n",
    "          f\"accuracy={accuracy:.2f}% ({correct}/{total})\\n\")\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            \"Q4_test_loss\": test_loss,\n",
    "            \"Q4_test_ppl\": test_ppl,\n",
    "            \"Q4_test_accuracy\": accuracy,\n",
    "        })\n",
    "\n",
    "    # ─── 6. Save predictions as TSV & CSV ────────────────────────────\n",
    "    preds_df = pd.DataFrame(all_preds, columns=[\"source\", \"target\", \"prediction\"])\n",
    "    tsv_path = Path(args.output_dir) / \"predictions.tsv\"\n",
    "    csv_path = Path(args.output_dir) / \"predictions.csv\"\n",
    "    preds_df.to_csv(tsv_path, sep=\"\\t\", index=False)\n",
    "    preds_df.to_csv(csv_path,            index=False)\n",
    "    print(f\"Saved all predictions → {tsv_path}, {csv_path}\\n\")\n",
    "\n",
    "    # ─── 7. Creative colored table of 20 examples ─────────────────────\n",
    "    sample_df = preds_df.sample(20, random_state=42)\n",
    "    colors = [[\"#c8e6c9\" if row[\"target\"]==row[\"prediction\"] else \"#ffcdd2\"\n",
    "               for _ in sample_df.columns] for _, row in sample_df.iterrows()]\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.axis(\"off\")\n",
    "    tbl = ax.table(cellText=sample_df.values.tolist(),\n",
    "                   colLabels=sample_df.columns.tolist(),\n",
    "                   cellColours=colors,\n",
    "                   cellLoc=\"center\", loc=\"center\")\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(12)\n",
    "    tbl.scale(1, 2)\n",
    "    if use_wandb:\n",
    "        wandb.log({\"Q4_sample_table\": wandb.Image(fig)})\n",
    "    figure_path = Path(args.output_dir) / \"sample_predictions.png\"\n",
    "    fig.savefig(figure_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved sample predictions figure to {figure_path}\\n\")\n",
    "\n",
    "    # ─── 8. Print markdown table ───────────────────────────────────────\n",
    "    # Print a properly aligned markdown table:\n",
    "    cols = [\"source\", \"target\", \"prediction\"]\n",
    "    # compute the max display width of each column\n",
    "    col_widths = {\n",
    "        col: max(\n",
    "            wcswidth(str(val)) for val in ([col] + sample_df[col].astype(str).tolist())\n",
    "        )\n",
    "        for col in cols\n",
    "    }\n",
    "\n",
    "    # header line\n",
    "    header = \"| \" + \" | \".join(\n",
    "        col.center(col_widths[col]) for col in cols\n",
    "    ) + \" |\"\n",
    "    sep = \"|-\" + \"-|-\".join(\"-\" * col_widths[col] for col in cols) + \"-|\"\n",
    "    print(header)\n",
    "    print(sep)\n",
    "\n",
    "    for _, row in sample_df.iterrows():\n",
    "        mark = \"✅\" if row[\"target\"] == row[\"prediction\"] else \"❌\"\n",
    "        line = \"| \" + \" | \".join(\n",
    "            # pad each cell on the right so it fills its column’s display width\n",
    "            row[col].ljust(col_widths[col] + (len(row[col]) - wcswidth(row[col])))\n",
    "            for col in cols\n",
    "        ) + f\" | {mark}\"\n",
    "        print(line)\n",
    "\n",
    "    # print(\"| source        | target         | prediction     |\")\n",
    "    # print(\"|---------------|----------------|----------------|\")\n",
    "    # for src, target, pred in sample_df.values.tolist():\n",
    "    #     mark = \"✅\" if target==pred else \"❌\"\n",
    "    #     print(f\"| {src:13} | {target:14} | {pred:14} | {mark}\")\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"solution_4.py\",\n",
    "    \"--train_tsv\", \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\",   \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\",  \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--checkpoint\", \"../checkpoints/best_seq2seq.pt\",\n",
    "    \"--output_dir\", \"../predictions_vanilla\",\n",
    "    \"--gpu_ids\", \"3\",\n",
    "    \"--wandb_project\", \"transliteration\",\n",
    "    \"--wandb_run_name\", \"solution_4_run\",\n",
    "    \"--wandb_run_tag\", \"solution_4\",\n",
    "]\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_jax_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
