{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce5c970",
   "metadata": {},
   "source": [
    "# Q5 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cddd52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "# ──────────────────── 2.5. Embedding-method modules ────────────────────\n",
    "class OneHotEmbedding(nn.Module):\n",
    "    \"\"\"Convert token ids → explicit one-hot → linear projection to embedding_dim.\"\"\"\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        # Project a one-hot vector of length vocab_size → embedding_dim\n",
    "        self.projection = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # token_ids: (B, T)\n",
    "        one_hot = F.one_hot(token_ids, num_classes=self.vocab_size).float()  # (B,T,V)\n",
    "        # zero out pad positions if desired\n",
    "        one_hot[token_ids == self.padding_idx] = 0.0\n",
    "        # project → (B, T, D)\n",
    "        return self.projection(one_hot)\n",
    "    \n",
    "class CharCNNEmbedding(nn.Module):\n",
    "    \"\"\"Convert token ids → one-hot → conv1d over time → (B,T,embedding_dim).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        kernel_size: int = 3,\n",
    "        num_filters: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_filters = num_filters or embedding_dim\n",
    "        # Convolution: in_channels=vocab_size, out_channels=num_filters\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=vocab_size,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            bias=False\n",
    "        )\n",
    "        # Optionally project filters → embedding_dim\n",
    "        if self.num_filters != embedding_dim:\n",
    "            self.projection = nn.Linear(self.num_filters, embedding_dim, bias=False)\n",
    "        else:\n",
    "            self.projection = None\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # token_ids: (B, T)\n",
    "        one_hot = F.one_hot(token_ids, num_classes=self.vocab_size).float()  # (B,T,V)\n",
    "        x = one_hot.permute(0, 2, 1)  # (B, V, T)\n",
    "        x = self.conv1d(x)           # (B, F, T)\n",
    "        x = x.permute(0, 2, 1)       # (B, T, F)\n",
    "        if self.projection:\n",
    "            x = self.projection(x)   # (B, T, D)\n",
    "        return x                     # (B, T, embedding_dim)\n",
    "    \n",
    "class SVDPPMIEmbedding(nn.Module):\n",
    "    \"\"\"Build PPMI→SVD char embeddings, then (if needed) project to embedding_dim.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seqs: List[List[int]],\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        window: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 1) Build co-occurrence counts\n",
    "        cooc = np.zeros((vocab_size, vocab_size), dtype=np.float64)\n",
    "        for seq in token_seqs:\n",
    "            for i, u in enumerate(seq):\n",
    "                if u == padding_idx: continue\n",
    "                for j in range(max(0, i - window), min(len(seq), i + window + 1)):\n",
    "                    if i == j: continue\n",
    "                    v = seq[j]\n",
    "                    if v == padding_idx: continue\n",
    "                    cooc[u, v] += 1\n",
    "\n",
    "        # 2) Compute PPMI matrix\n",
    "        total = cooc.sum()\n",
    "        row_sums = cooc.sum(axis=1, keepdims=True)\n",
    "        col_sums = cooc.sum(axis=0, keepdims=True)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            pmi = np.log((cooc * total) / (row_sums * col_sums))\n",
    "        pmi[np.isnan(pmi)] = 0.0\n",
    "        pmi[pmi < 0] = 0.0\n",
    "\n",
    "        # 3) Truncated SVD\n",
    "        U, S, _ = np.linalg.svd(pmi, full_matrices=False)\n",
    "        # D0 is the actual SVD dimension we get (≤ vocab_size)\n",
    "        D0 = min(embedding_dim, U.shape[1])\n",
    "        U = U[:, :D0]            # (vocab_size, D0)\n",
    "        S = S[:D0]               # (D0,)\n",
    "        emb_matrix = U * np.sqrt(S)  # (vocab_size, D0)\n",
    "\n",
    "        # Register static SVD weights\n",
    "        self.register_buffer(\"weight\", torch.from_numpy(emb_matrix).float())\n",
    "\n",
    "        # 4) If the SVD rank D0 is smaller than requested embedding_dim, add a projection\n",
    "        if D0 < embedding_dim:\n",
    "            # Warn the user clearly\n",
    "            warnings.warn(\n",
    "                f\"SVD/PPMI yielded only {D0} dimensions (≤ vocab size), \"\n",
    "                f\"but embedding_size={embedding_dim} was requested. \"\n",
    "                \"Adding a learnable linear projection to expand from \"\n",
    "                f\"{D0} → {embedding_dim} dimensions.\",\n",
    "                UserWarning\n",
    "            )\n",
    "            self.expander = nn.Linear(D0, embedding_dim, bias=False)\n",
    "        else:\n",
    "            self.expander = None\n",
    "\n",
    "    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        # Lookup static SVD embeddings → (B, T, D0)\n",
    "        x = F.embedding(token_ids, self.weight, padding_idx=self.weight.new_zeros(1).long())\n",
    "        # If we have an expander, project to the full embedding_dim\n",
    "        if self.expander:\n",
    "            x = self.expander(x)\n",
    "        return x  # (B, T, embedding_dim)\n",
    "    \n",
    "\n",
    "# map a string to the corresponding nn.RNN module\n",
    "_RNN_MAP: Dict[str, nn.Module] = {\n",
    "    \"RNN\": nn.RNN,\n",
    "    \"LSTM\": nn.LSTM,\n",
    "    \"GRU\": nn.GRU,\n",
    "}\n",
    "\n",
    "# ─────────────────────── 1. Vocabulary helpers ───────────────────────\n",
    "SPECIAL_TOKENS = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
    "\n",
    "\n",
    "# ─────────────────────── 1. Vocabulary helpers ───────────────────────\n",
    "SPECIAL_TOKENS = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
    "\n",
    "\n",
    "class CharVocabulary:\n",
    "    \"\"\"Character-level vocabulary that handles <pad>, <sos>, and <eos>.\"\"\"\n",
    "    def __init__(self, characters: List[str]):\n",
    "        unique_chars = sorted(set(characters))\n",
    "        # string→index and index→string maps\n",
    "        self.stoi: Dict[str, int] = {\n",
    "            **SPECIAL_TOKENS,\n",
    "            **{ch: idx + len(SPECIAL_TOKENS) for idx, ch in enumerate(unique_chars)}\n",
    "        }\n",
    "        self.itos: Dict[int, str] = {idx: ch for ch, idx in self.stoi.items()}\n",
    "\n",
    "    def encode(self, text: str, *, add_sos: bool = False, add_eos: bool = True) -> List[int]:\n",
    "        \"\"\"Convert a string to list of token ids (with optional <sos>/<eos>).\"\"\"\n",
    "        ids = [self.stoi[ch] for ch in text]\n",
    "        if add_eos:\n",
    "            ids.append(self.stoi[\"<eos>\"])\n",
    "        if add_sos:\n",
    "            ids.insert(0, self.stoi[\"<sos>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert ids back to string (stop at <eos>).\"\"\"\n",
    "        chars: List[str] = []\n",
    "        for idx in ids:\n",
    "            if idx == self.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            chars.append(self.itos.get(idx, \"\"))\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of tokens in vocabulary.\"\"\"\n",
    "        return len(self.stoi)\n",
    "\n",
    "# ─────────────────────────── Helper: Align Hidden State ──────────────────────────\n",
    "def _align_hidden_state(hidden_state, target_num_layers: int):\n",
    "    \"\"\"\n",
    "    Adjust the encoder's final hidden state to match the decoder's expected\n",
    "    number of layers. Works for both LSTM (tuple of (h,c)) and GRU/RNN (single tensor).\n",
    "    \n",
    "    Strategies:\n",
    "    - If encoder_layers == decoder_layers: return hidden_state unchanged.\n",
    "    - If encoder_layers  > decoder_layers: take the **last** `target_num_layers` layers.\n",
    "    - If encoder_layers  < decoder_layers: **repeat** the final layer's state\n",
    "      so that the total number of layers equals `target_num_layers`.\n",
    "    \"\"\"\n",
    "    def _repeat_last_layer(tensor, repeat_count: int):\n",
    "        # tensor shape: (enc_layers, batch_size, hidden_dim)\n",
    "        last_layer = tensor[-1:]                             # shape: (1, B, H)\n",
    "        repeated   = last_layer.expand(repeat_count, -1, -1) # shape: (repeat_count, B, H)\n",
    "        return torch.cat([tensor, repeated], dim=0)          # new shape: (enc_layers+repeat_count, B, H)\n",
    "\n",
    "    if isinstance(hidden_state, tuple):\n",
    "        h, c = hidden_state\n",
    "        enc_layers, batch_size, hid_dim = h.shape\n",
    "\n",
    "        if enc_layers == target_num_layers:\n",
    "            return h, c\n",
    "        \n",
    "        # Warn whenever we need to truncate or repeat\n",
    "        warnings.warn(\n",
    "            f\"Encoder has {enc_layers} layers but decoder expects {target_num_layers}. \"\n",
    "            f\"{'Truncating' if enc_layers>target_num_layers else 'Repeating last layer'} hidden state.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "        if enc_layers > target_num_layers:\n",
    "            # keep only the last `target_num_layers` layers\n",
    "            return h[-target_num_layers:], c[-target_num_layers:]\n",
    "        else:\n",
    "            # repeat the final layer's state to pad up to target_num_layers\n",
    "            to_repeat = target_num_layers - enc_layers\n",
    "            return _repeat_last_layer(h, to_repeat), _repeat_last_layer(c, to_repeat)\n",
    "    else:\n",
    "        h = hidden_state\n",
    "        enc_layers, batch_size, hid_dim = h.shape\n",
    "\n",
    "        if enc_layers == target_num_layers:\n",
    "            return h\n",
    "        \n",
    "        # Same warning for single‐tensor hidden states\n",
    "        warnings.warn(\n",
    "            f\"Encoder has {enc_layers} layers but decoder expects {target_num_layers}. \"\n",
    "            f\"{'Truncating' if enc_layers>target_num_layers else 'Repeating last layer'} hidden state.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "        if enc_layers > target_num_layers:\n",
    "            return h[-target_num_layers:]\n",
    "        else:\n",
    "            to_repeat = target_num_layers - enc_layers\n",
    "            return _repeat_last_layer(h, to_repeat)\n",
    "        \n",
    "\n",
    "# ───────────────────────── 2. Dataset class ─────────────────────────\n",
    "class DakshinaLexicon(Dataset):\n",
    "    \"\"\"Loads a Dakshina *lexicon* TSV and encodes (source, target) pairs.\n",
    "\n",
    "    TSV columns: native_word, romanized_word, count\n",
    "    We treat romanized_word as source and native_word as target.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tsv_path: str | Path,\n",
    "        source_vocab: Optional[CharVocabulary] = None,\n",
    "        target_vocab: Optional[CharVocabulary] = None,\n",
    "        *,\n",
    "        build_vocabs: bool = False,\n",
    "        use_attestations: bool = False\n",
    "    ):\n",
    "        # Read TSV – three columns, ensure correct dtypes\n",
    "        dataframe = pd.read_csv(\n",
    "            tsv_path, sep=\"\\t\", header=None,\n",
    "            names=[\"target_native\", \"source_roman\", \"count\"],\n",
    "            dtype={\"target_native\": str, \"source_roman\": str, \"count\": int}\n",
    "        ).dropna()\n",
    "\n",
    "        # Optionally keep annotator counts for sampling or weighting\n",
    "        self.example_counts: Optional[List[int]] = (\n",
    "            dataframe[\"count\"].tolist() if use_attestations else None\n",
    "        )\n",
    "\n",
    "        # Keep only the (src, tgt) pairs\n",
    "        self.word_pairs: List[Tuple[str, str]] = list(zip(\n",
    "            dataframe[\"source_roman\"], dataframe[\"target_native\"]\n",
    "        ))\n",
    "\n",
    "        # Build new or reuse provided vocabularies\n",
    "        if build_vocabs:\n",
    "            assert source_vocab is None and target_vocab is None, (\n",
    "                \"Cannot pass existing vocabs when build_vocabs=True\"\n",
    "            )\n",
    "            # collect all chars\n",
    "            all_src_chars = [ch for src, _ in self.word_pairs for ch in src]\n",
    "            all_tgt_chars = [ch for _, tgt in self.word_pairs for ch in tgt]\n",
    "            source_vocab = CharVocabulary(all_src_chars)\n",
    "            target_vocab = CharVocabulary(all_tgt_chars)\n",
    "\n",
    "        assert source_vocab is not None and target_vocab is not None, (\n",
    "            \"Must provide or build vocabularies\"\n",
    "        )\n",
    "        self.src_vocab, self.tgt_vocab = source_vocab, target_vocab\n",
    "\n",
    "        # Encode all pairs once for efficiency\n",
    "        self.encoded_pairs: List[Tuple[List[int], List[int]]] = [\n",
    "            (self.src_vocab.encode(src),\n",
    "             self.tgt_vocab.encode(tgt, add_sos=True))\n",
    "            for src, tgt in self.word_pairs\n",
    "        ]\n",
    "        # Also keep just the source sequences for SVD/PPMI embedding\n",
    "        self.encoded_sources: List[List[int]] = [src for src, _ in self.encoded_pairs]\n",
    "\n",
    "        # Padding token id\n",
    "        self.pad_id: int = self.src_vocab.stoi[\"<pad>\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded_pairs)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[List[int], List[int]]:\n",
    "        return self.encoded_pairs[index]\n",
    "\n",
    "def collate_batch(\n",
    "    batch: List[Tuple[List[int], List[int]]],\n",
    "    pad_id: int\n",
    ") -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Pad source and target sequences to uniform length within a batch.\"\"\"\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(s) for s in src_seqs], dtype=torch.long)\n",
    "    tgt_lengths = torch.tensor([len(t) for t in tgt_seqs], dtype=torch.long)\n",
    "\n",
    "    max_src_len = src_lengths.max().item()\n",
    "    max_tgt_len = tgt_lengths.max().item()\n",
    "\n",
    "    padded_sources = torch.full((len(batch), max_src_len), pad_id, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(batch), max_tgt_len), pad_id, dtype=torch.long)\n",
    "\n",
    "    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
    "        padded_sources[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "        padded_targets[i, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "\n",
    "    return padded_sources, src_lengths, padded_targets\n",
    "\n",
    "# ───────────────────────── 7. Training & evaluation ─────────────────────────\n",
    "def train_epoch(\n",
    "    model: Seq2Seq,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    device: str,\n",
    "    teacher_forcing: float\n",
    ") -> float:\n",
    "    \"\"\"Run one epoch of training; return average token-level cross-entropy loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for src, src_len, tgt in loader:\n",
    "        src, src_len, tgt = src.to(device), src_len.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, src_len, tgt, teacher_forcing_ratio=teacher_forcing)\n",
    "        # shift tgt so we predict t when truth is at t\n",
    "        gold = tgt[:, 1:].contiguous()\n",
    "        preds = logits[:, 1:].contiguous()\n",
    "        loss = loss_fn(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_valid = (gold != loss_fn.ignore_index).sum().item()\n",
    "        total_loss += loss.item() * n_valid\n",
    "        total_tokens += n_valid\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def eval_epoch(\n",
    "    model: Seq2Seq,\n",
    "    loader: DataLoader,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    device: str\n",
    ") -> float:\n",
    "    \"\"\"Run one epoch of evaluation; return average token-level cross-entropy loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_len, tgt in loader:\n",
    "            src, src_len, tgt = src.to(device), src_len.to(device), tgt.to(device)\n",
    "            logits = model(src, src_len, tgt, teacher_forcing_ratio=0.0)\n",
    "            gold = tgt[:, 1:].contiguous()\n",
    "            preds = logits[:, 1:].contiguous()\n",
    "            loss = loss_fn(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "            n_valid = (gold != loss_fn.ignore_index).sum().item()\n",
    "            total_loss += loss.item() * n_valid\n",
    "            total_tokens += n_valid\n",
    "\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e7ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_5_model.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Attention‐augmented Seq2Seq for Dakshina Hindi transliteration (Q5).\n",
    "\n",
    "This module defines:\n",
    "  - Seq2SeqAttentionConfig: hyper‐parameters & model options\n",
    "  - DotProductAttention: simple global dot‐product attention\n",
    "  - EncoderWithOutputs: returns per‐time‐step features + final hidden\n",
    "  - DecoderWithAttention: attends + decodes one token at a time\n",
    "  - Seq2SeqAttention: end‐to‐end training / inference wrapper\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────── 1. Configuration ─────────────────────────\n",
    "@dataclass\n",
    "class Seq2SeqAttentionConfig:\n",
    "    source_vocab_size: int\n",
    "    target_vocab_size: int\n",
    "\n",
    "    embedding_dim: int = 256\n",
    "    hidden_dim:    int = 512\n",
    "\n",
    "    encoder_layers: int = 1\n",
    "    decoder_layers: int = 1\n",
    "\n",
    "    cell_type: str = \"LSTM\"       # \"RNN\" | \"GRU\" | \"LSTM\"\n",
    "    dropout:   float = 0.1\n",
    "\n",
    "    pad_index: int = 0\n",
    "    sos_index: int = 1\n",
    "    eos_index: int = 2\n",
    "\n",
    "    embedding_method: str = \"learned\"  # \"learned\" | \"onehot\" | \"char_cnn\" | \"svd_ppmi\"\n",
    "    svd_sources: Optional[List[List[int]]] = None\n",
    "\n",
    "    attention_dim: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.cell_type in {\"RNN\", \"GRU\", \"LSTM\"}, \"cell_type must be RNN, GRU or LSTM\"\n",
    "        assert self.embedding_method in {\"learned\", \"onehot\", \"char_cnn\", \"svd_ppmi\"}\n",
    "        if self.embedding_method == \"svd_ppmi\":\n",
    "            assert self.svd_sources is not None, \"svd_ppmi requires svd_sources\"\n",
    "        if self.attention_dim is None:\n",
    "            self.attention_dim = self.hidden_dim\n",
    "\n",
    "\n",
    "# ───────────────────────── 2. Encoder ───────────────────────────────\n",
    "class EncoderWithOutputs(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder that returns both:\n",
    "      - outputs      : (B, T_src, hidden_dim)\n",
    "      - hidden_state : final hidden state(s) for decoder init\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # character embedding\n",
    "        if cfg.embedding_method == \"learned\":\n",
    "            self.embedding = nn.Embedding(cfg.source_vocab_size,\n",
    "                                          cfg.embedding_dim,\n",
    "                                          padding_idx=cfg.pad_index)\n",
    "        elif cfg.embedding_method == \"onehot\":\n",
    "            self.embedding = OneHotEmbedding(\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "        elif cfg.embedding_method == \"char_cnn\":\n",
    "            self.embedding = CharCNNEmbedding(\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "        else:  # \"svd_ppmi\"\n",
    "            self.embedding = SVDPPMIEmbedding(\n",
    "                token_seqs=cfg.svd_sources,\n",
    "                vocab_size=cfg.source_vocab_size,\n",
    "                embedding_dim=cfg.embedding_dim,\n",
    "                padding_idx=cfg.pad_index\n",
    "            )\n",
    "\n",
    "        # RNN stack\n",
    "        RNNClass = _RNN_MAP[cfg.cell_type]\n",
    "        self.rnn = RNNClass(\n",
    "            input_size=cfg.embedding_dim,\n",
    "            hidden_size=cfg.hidden_dim,\n",
    "            num_layers=cfg.encoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=cfg.dropout if cfg.encoder_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor\n",
    "    ) -> Tuple[torch.Tensor, Any]:\n",
    "        # embed: (B, T_src, D_emb)\n",
    "        embedded = self.embedding(src)\n",
    "\n",
    "        # pack for RNN\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_outputs, hidden_state = self.rnn(packed)\n",
    "\n",
    "        # unpack to (B, T_src, hidden_dim)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outputs,\n",
    "            batch_first=True,\n",
    "            padding_value=self.cfg.pad_index\n",
    "        )\n",
    "        return outputs, hidden_state\n",
    "\n",
    "\n",
    "# ───────────────────────── 3. Attention ─────────────────────────────\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Global dot-product attention:\n",
    "      score_{t,i} = h_dec(t) · h_enc(i)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        decoder_hidden: torch.Tensor,   # (B, hidden_dim)\n",
    "        encoder_outputs: torch.Tensor,  # (B, T_src, hidden_dim)\n",
    "        mask: torch.Tensor              # (B, T_src)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # scores: (B, T_src)\n",
    "        scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        alignments = F.softmax(scores, dim=1)                 # (B, T_src)\n",
    "        context    = torch.bmm(alignments.unsqueeze(1),      # (B,1,T_src)\n",
    "                               encoder_outputs\n",
    "                              ).squeeze(1)                   # (B, hidden_dim)\n",
    "        return context, alignments\n",
    "\n",
    "\n",
    "# ───────────────────────── 4. Decoder ───────────────────────────────\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder RNN that at each step:\n",
    "      1) embeds input\n",
    "      2) attends over encoder_outputs\n",
    "      3) feeds [embed; context] to RNN\n",
    "      4) projects to vocab logits\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embedding = nn.Embedding(cfg.target_vocab_size,\n",
    "                                      cfg.embedding_dim,\n",
    "                                      padding_idx=cfg.pad_index)\n",
    "        self.attention = DotProductAttention()\n",
    "\n",
    "        rnn_input_size = cfg.embedding_dim + cfg.hidden_dim\n",
    "        RNNClass = _RNN_MAP[cfg.cell_type]\n",
    "        self.rnn = RNNClass(\n",
    "            input_size=rnn_input_size,\n",
    "            hidden_size=cfg.hidden_dim,\n",
    "            num_layers=cfg.decoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=cfg.dropout if cfg.decoder_layers > 1 else 0.0\n",
    "        )\n",
    "        self.output_projection = nn.Linear(cfg.hidden_dim,\n",
    "                                           cfg.target_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token: torch.LongTensor,    # (B,1)\n",
    "        last_hidden: Any,\n",
    "        encoder_outputs: torch.Tensor,    # (B, T_src, hidden_dim)\n",
    "        encoder_mask: torch.Tensor        # (B, T_src)\n",
    "    ) -> Tuple[torch.Tensor, Any, torch.Tensor]:\n",
    "        # embed: (B,1,D_emb)\n",
    "        emb = self.embedding(input_token)\n",
    "\n",
    "        # extract last layer of hidden\n",
    "        if isinstance(last_hidden, tuple):\n",
    "            dec_h = last_hidden[0][-1]  # (B, hidden_dim)\n",
    "        else:\n",
    "            dec_h = last_hidden[-1]     # (B, hidden_dim)\n",
    "\n",
    "        # attention\n",
    "        context, alignments = self.attention(dec_h,\n",
    "                                             encoder_outputs,\n",
    "                                             encoder_mask)\n",
    "        # concat: (B,1, D_emb+hidden_dim)\n",
    "        rnn_in = torch.cat([emb, context.unsqueeze(1)], dim=2)\n",
    "        output, new_hidden = self.rnn(rnn_in, last_hidden)  # (B,1,hidden_dim)\n",
    "        logits = self.output_projection(output)              # (B,1,V)\n",
    "        return logits, new_hidden, alignments\n",
    "\n",
    "\n",
    "# ──────────────────── 5. Seq2SeqAttention ─────────────────────────\n",
    "class Seq2SeqAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Full encoder-decoder with attention.\n",
    "    Supports forward, greedy_decode, and beam_search_decode.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Seq2SeqAttentionConfig):\n",
    "        super().__init__()\n",
    "        self.cfg     = cfg\n",
    "        self.encoder = EncoderWithOutputs(cfg)\n",
    "        self.decoder = DecoderWithAttention(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor,\n",
    "        tgt: torch.LongTensor,\n",
    "        *,\n",
    "        teacher_forcing_ratio: float = 0.5\n",
    "    ) -> torch.Tensor:\n",
    "        B, T_tgt = tgt.size()\n",
    "        device   = src.device\n",
    "        enc_outputs, enc_hidden = self.encoder(src, src_lengths)\n",
    "        enc_mask  = (src != self.cfg.pad_index).to(device)\n",
    "        dec_hidden = _align_hidden_state(enc_hidden,\n",
    "                                        self.cfg.decoder_layers)\n",
    "\n",
    "        logits_all = torch.zeros(B, T_tgt,\n",
    "                                 self.cfg.target_vocab_size,\n",
    "                                 device=device)\n",
    "        dec_input  = tgt[:, 0].unsqueeze(1)  # (B,1)\n",
    "\n",
    "        for t in range(1, T_tgt):\n",
    "            step_logits, dec_hidden, _ = self.decoder(\n",
    "                dec_input, dec_hidden,\n",
    "                enc_outputs, enc_mask\n",
    "            )\n",
    "            logits_all[:, t] = step_logits.squeeze(1)\n",
    "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                dec_input = tgt[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                dec_input = step_logits.argmax(-1)\n",
    "\n",
    "        return logits_all\n",
    "\n",
    "    def greedy_decode(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor,\n",
    "        *,\n",
    "        max_len: int = 50\n",
    "    ) -> torch.LongTensor:\n",
    "        B = src.size(0)\n",
    "        device = src.device\n",
    "        enc_outputs, enc_hidden = self.encoder(src, src_lengths)\n",
    "        enc_mask = (src != self.cfg.pad_index).to(device)\n",
    "        dec_hidden = _align_hidden_state(enc_hidden,\n",
    "                                        self.cfg.decoder_layers)\n",
    "\n",
    "        dec_input = torch.full((B,1),\n",
    "                               self.cfg.sos_index,\n",
    "                               device=device,\n",
    "                               dtype=torch.long)\n",
    "        generated = []\n",
    "        for _ in range(max_len):\n",
    "            logits, dec_hidden, _ = self.decoder(\n",
    "                dec_input, dec_hidden,\n",
    "                enc_outputs, enc_mask\n",
    "            )\n",
    "            dec_input = logits.argmax(-1)  # (B,1)\n",
    "            generated.append(dec_input)\n",
    "        return torch.cat(generated, dim=1)  # (B, max_len)\n",
    "\n",
    "    def beam_search_decode(\n",
    "        self,\n",
    "        src: torch.LongTensor,\n",
    "        src_lengths: torch.LongTensor,\n",
    "        *,\n",
    "        beam_size: int = 5,\n",
    "        max_len:   int = 50\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Beam-search decoding (batch_size=1 only).\n",
    "        Returns best seq (1, L) without leading <sos>.\n",
    "        \"\"\"\n",
    "        B = src.size(0)\n",
    "        assert B == 1, \"beam_search_decode only supports batch_size=1\"\n",
    "\n",
    "        device = src.device\n",
    "        enc_outputs, enc_hidden = self.encoder(src, src_lengths)\n",
    "        enc_mask  = (src != self.cfg.pad_index).to(device)\n",
    "        dec_hidden = _align_hidden_state(enc_hidden,\n",
    "                                        self.cfg.decoder_layers)\n",
    "\n",
    "        # beams: list of (token_list, score, hidden_state)\n",
    "        beams = [([self.cfg.sos_index], 0.0, dec_hidden)]\n",
    "        completed = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score, hidden in beams:\n",
    "                last = seq[-1]\n",
    "                if last == self.cfg.eos_index:\n",
    "                    completed.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                inp = torch.tensor([[last]],\n",
    "                                   device=device)\n",
    "                logits, new_hidden, _ = self.decoder(\n",
    "                    inp, hidden,\n",
    "                    enc_outputs, enc_mask\n",
    "                )\n",
    "                # logits: (1,1,V) → (V,)\n",
    "                log_probs = F.log_softmax(logits.squeeze(1), dim=-1)[0]\n",
    "\n",
    "                topk_vals, topk_idx = log_probs.topk(beam_size)\n",
    "                for lp, idx in zip(topk_vals.tolist(),\n",
    "                                   topk_idx.tolist()):\n",
    "                    # detach hidden state for this candidate\n",
    "                    if isinstance(new_hidden, tuple):\n",
    "                        h, c = new_hidden\n",
    "                        nh = (h.detach().clone(),\n",
    "                              c.detach().clone())\n",
    "                    else:\n",
    "                        nh = new_hidden.detach().clone()\n",
    "                    all_candidates.append((\n",
    "                        seq + [idx],\n",
    "                        score + lp,\n",
    "                        nh\n",
    "                    ))\n",
    "\n",
    "            if not all_candidates:\n",
    "                break\n",
    "\n",
    "            # keep top beam_size\n",
    "            beams = sorted(all_candidates,\n",
    "                           key=lambda x: x[1],\n",
    "                           reverse=True)[:beam_size]\n",
    "            # if all beams ended, stop early\n",
    "            if all(b[-1] == self.cfg.eos_index for b, _, _ in beams):\n",
    "                completed.extend((b, s) for b, s, _ in beams)\n",
    "                break\n",
    "\n",
    "        if not completed:\n",
    "            completed = [(b, s) for b, s, _ in beams]\n",
    "\n",
    "        # pick best\n",
    "        best_seq, _ = max(completed, key=lambda x: x[1])\n",
    "        # drop leading <sos>\n",
    "        if best_seq and best_seq[0] == self.cfg.sos_index:\n",
    "            best_seq = best_seq[1:]\n",
    "        return torch.tensor(best_seq,\n",
    "                            dtype=torch.long,\n",
    "                            device=device).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q5: W&B sweep driver for attention‐augmented Seq2Seq\n",
    "────────────────────────────────────────────────────────────────\n",
    "Reuses the attention Seq2Seq from solution_5_model.py.\n",
    "A YAML under ./configs/ specifies the sweep space.\n",
    "\n",
    "Usage:\n",
    "    # Sweep:\n",
    "    python solution_5.py \\\n",
    "      --mode sweep \\\n",
    "      --sweep_config sweep_attention.yaml \\\n",
    "      --wandb_project DA6401_Intro_to_DeepLearning_Assignment_3 \\\n",
    "      --wandb_run_tag solution_5 \\\n",
    "      --gpu_ids 0 2 3 \\\n",
    "      --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "      --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "      --test_tsv  ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "      --sweep_count 75\n",
    "\n",
    "    # Single debug run:\n",
    "    python solution_5.py \\\n",
    "      --mode single \\\n",
    "      --wandb_project transliteration \\\n",
    "      --wandb_run_tag attention_debug \\\n",
    "      --train_tsv ... \\\n",
    "      --dev_tsv   ... \\\n",
    "      --test_tsv  ...\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_configs(project_root: str | Path, config_filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load a YAML sweep configuration from ./configs/.\"\"\"\n",
    "    cfg_path = Path(project_root) / \"configs\" / config_filename\n",
    "    with open(cfg_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        return yaml.safe_load(handle)\n",
    "\n",
    "\n",
    "def compute_sequence_accuracy(\n",
    "    model: Seq2SeqAttention,\n",
    "    dataset: DakshinaLexicon,\n",
    "    device: str,\n",
    "    beam_size: int = 1\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Exact‐match accuracy over the dataset using beam-search.\n",
    "    Strips leading <sos> from predictions before decoding.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src_ids, tgt_ids in dataset:\n",
    "            src_tensor = torch.tensor([src_ids], device=device)\n",
    "            src_len    = torch.tensor([len(src_ids)], device=device)\n",
    "\n",
    "            pred = model.beam_search_decode(\n",
    "                src_tensor, src_len,\n",
    "                beam_size=beam_size,\n",
    "                max_len=len(tgt_ids)\n",
    "            )[0]  # (L,)\n",
    "\n",
    "            pred_list = pred.tolist()\n",
    "            # drop leading <sos>\n",
    "            sos_idx = dataset.tgt_vocab.stoi[\"<sos>\"]\n",
    "            if pred_list and pred_list[0] == sos_idx:\n",
    "                pred_list = pred_list[1:]\n",
    "\n",
    "            pred_str = dataset.tgt_vocab.decode(pred_list)\n",
    "            gold_str = dataset.tgt_vocab.decode(tgt_ids[1:])\n",
    "\n",
    "            correct += int(pred_str == gold_str)\n",
    "            total   += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def run_single_training(sweep_config: Dict[str, Any], static_args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Train + evaluate once using hyperparams in sweep_config\n",
    "    and fixed filepaths and tags from CLI.\n",
    "    \"\"\"\n",
    "    # pin GPUs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(g) for g in static_args.gpu_ids)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Data\n",
    "    train_ds = DakshinaLexicon(\n",
    "        static_args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=sweep_config.get(\"use_attestations\", False)\n",
    "    )\n",
    "    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n",
    "    dev_ds  = DakshinaLexicon(static_args.dev_tsv,  src_vocab, tgt_vocab)\n",
    "    test_ds = DakshinaLexicon(static_args.test_tsv, src_vocab, tgt_vocab)\n",
    "\n",
    "    collate_fn = lambda batch: collate_batch(batch,\n",
    "                                             pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              batch_size=sweep_config[\"batch_size\"],\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "    dev_loader   = DataLoader(dev_ds,\n",
    "                              batch_size=sweep_config[\"batch_size\"],\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,\n",
    "                              batch_size=sweep_config[\"batch_size\"],\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "    # Model + optimizer + loss\n",
    "    extra = {}\n",
    "    if sweep_config[\"embedding_method\"] == \"svd_ppmi\":\n",
    "        extra[\"svd_sources\"] = train_ds.encoded_sources\n",
    "\n",
    "    cfg = Seq2SeqAttentionConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=sweep_config[\"embedding_size\"],\n",
    "        hidden_dim=sweep_config[\"hidden_size\"],\n",
    "        encoder_layers=sweep_config[\"encoder_layers\"],\n",
    "        decoder_layers=sweep_config[\"decoder_layers\"],\n",
    "        cell_type=sweep_config[\"cell\"],\n",
    "        dropout=sweep_config[\"dropout\"],\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=sweep_config[\"embedding_method\"],\n",
    "        **extra\n",
    "    )\n",
    "    model     = Seq2SeqAttention(cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=sweep_config[\"learning_rate\"])\n",
    "    loss_fn   = torch.nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    # build a run name from hyperparams\n",
    "    run_name = (\n",
    "        f\"emb:{sweep_config['embedding_method']}|es:{sweep_config['embedding_size']}|\"\n",
    "        f\"cell:{sweep_config['cell']}|hs:{sweep_config['hidden_size']}|\"\n",
    "        f\"enc:{sweep_config['encoder_layers']}|dec:{sweep_config['decoder_layers']}|\"\n",
    "        f\"dr:{sweep_config['dropout']}|lr:{sweep_config['learning_rate']:.1e}|\"\n",
    "        f\"bsz:{sweep_config['batch_size']}|tf:{sweep_config['teacher_forcing']}|\"\n",
    "        f\"ep:{sweep_config['epochs']}|beam:{sweep_config.get('beam_size',1)}|\"\n",
    "        f\"att:{sweep_config.get('use_attestations',False)}\"\n",
    "    )\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.tags = [static_args.wandb_run_tag]\n",
    "\n",
    "    # Train / validate\n",
    "    for epoch in range(1, sweep_config[\"epochs\"] + 1):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss_fn,\n",
    "            device, sweep_config[\"teacher_forcing\"]\n",
    "        )\n",
    "        dev_loss = eval_epoch(model, dev_loader, loss_fn, device)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Q5_epoch\":       epoch,\n",
    "            \"Q5_train_loss\":  train_loss,\n",
    "            \"Q5_train_ppl\":   math.exp(train_loss),\n",
    "            \"Q5_dev_loss\":    dev_loss,\n",
    "            \"Q5_dev_ppl\":     math.exp(dev_loss),\n",
    "        })\n",
    "\n",
    "    # final dev accuracy via beam-search\n",
    "    beam = sweep_config.get(\"beam_size\", 1)\n",
    "    dev_acc = compute_sequence_accuracy(model, dev_ds,\n",
    "                                        device, beam_size=beam)\n",
    "    wandb.log({\"Q5_dev_accuracy\": dev_acc})\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"W&B sweep driver for Q5 attention model.\"\n",
    "    )\n",
    "    parser.add_argument(\"--mode\",         choices=[\"sweep\",\"single\"], required=True)\n",
    "    parser.add_argument(\"--sweep_config\", type=str, default=\"sweep_attention.yaml\")\n",
    "    parser.add_argument(\"--wandb_project\",type=str, required=True)\n",
    "    parser.add_argument(\"--wandb_run_tag\",type=str, default=\"attention\")\n",
    "    parser.add_argument(\"--gpu_ids\",      nargs=\"+\", type=int, default=[0])\n",
    "    parser.add_argument(\"--train_tsv\",    type=str, required=True)\n",
    "    parser.add_argument(\"--dev_tsv\",      type=str, required=True)\n",
    "    parser.add_argument(\"--test_tsv\",     type=str, required=True)\n",
    "    parser.add_argument(\"--sweep_count\",  type=int, default=30)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    project_root = Path.cwd().resolve().parent\n",
    "    sweep_yaml   = get_configs(project_root, args.sweep_config)\n",
    "\n",
    "    if args.mode == \"sweep\":\n",
    "        # ensure metric, method, etc.\n",
    "        sweep_yaml.setdefault(\"method\", \"bayes\")\n",
    "        sweep_yaml.setdefault(\"metric\", {\"name\":\"dev_perplexity\",\"goal\":\"minimize\"})\n",
    "        sweep_yaml.setdefault(\"parameters\", sweep_yaml.get(\"parameters\",{}))\n",
    "        sweep_yaml[\"run_cap\"]  = args.sweep_count\n",
    "\n",
    "        sweep_id = wandb.sweep(sweep=sweep_yaml,\n",
    "                               project=args.wandb_project)\n",
    "        print(f\"Registered sweep: {sweep_id}\")\n",
    "\n",
    "        def _agent():\n",
    "            with wandb.init(project=args.wandb_project) as run:\n",
    "                run_single_training(dict(run.config), args)\n",
    "\n",
    "        wandb.agent(sweep_id, function=_agent,\n",
    "                    count=args.sweep_count)\n",
    "\n",
    "    else:\n",
    "        # single debug run\n",
    "        with wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            config=sweep_yaml.get(\"parameters\", {})\n",
    "        ) as run:\n",
    "            run.config.update({\n",
    "                \"epochs\":            3,\n",
    "                \"batch_size\":       64,\n",
    "                \"embedding_method\":\"learned\",\n",
    "                \"embedding_size\":   64,\n",
    "                \"hidden_size\":     128,\n",
    "                \"encoder_layers\":    1,\n",
    "                \"decoder_layers\":    1,\n",
    "                \"cell\":            \"LSTM\",\n",
    "                \"dropout\":          0.1,\n",
    "                \"learning_rate\":   1e-3,\n",
    "                \"teacher_forcing\":  0.5,\n",
    "                \"use_attestations\":False,\n",
    "                \"beam_size\":        1,\n",
    "            }, allow_val_change=True)\n",
    "            run_single_training(dict(run.config), args)\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simulate the exact CLI you’d type:\n",
    "sys.argv = [\n",
    "    \"solution_5.py\",\n",
    "    \"--mode\", \"sweep\",\n",
    "    \"--sweep_config\", \"sweep_attention.yaml\",\n",
    "    \"--wandb_project\", \"transliteration\",\n",
    "    \"--wandb_run_tag\", \"solution_5\",\n",
    "    \"--gpu_ids\", \"0\", \"2\", \"3\",\n",
    "    \"--train_tsv\", \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\",   \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\",  \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--sweep_count\", \"75\",\n",
    "]\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4243a26",
   "metadata": {},
   "source": [
    "#### Q5 Solution - Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "solution_5a.py: Tune beam size for your pretrained attention‐based Hindi transliteration model.\n",
    "\n",
    "Usage example:\n",
    "    python solution_5a.py \\\n",
    "        --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "        --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "        --test_tsv  ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "        --gpu_ids   0 1\n",
    "\n",
    "This script:\n",
    "  1. Loads the Dakshina Hindi train/dev/test splits.\n",
    "  2. Trains a Seq2SeqAttention model with your best attention hyperparameters.\n",
    "  3. Runs beam‐search decoding on the dev set for various beam sizes.\n",
    "  4. Reports exact‐match accuracy for each beam size.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ──────────────────────────── best hyperparameters ────────────────────────────\n",
    "best = {\n",
    "    \"batch_size\":        128,\n",
    "    \"cell_type\":       \"GRU\",\n",
    "    \"encoder_layers\":     1,\n",
    "    \"decoder_layers\":     2,\n",
    "    \"hidden_size\":      512,\n",
    "    \"embedding_method\":\"svd_ppmi\",\n",
    "    \"embedding_size\":   64,\n",
    "    \"dropout\":          0.2,\n",
    "    \"learning_rate\":   0.0006899910999897612,\n",
    "    \"teacher_forcing\":  0.5,\n",
    "    \"use_attestations\": True,\n",
    "    \"epochs\":           10,\n",
    "}\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Tune beam size for your best attention‐based model\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--train_tsv\",  type=str, required=True,\n",
    "        help=\"Path to Hindi training lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--dev_tsv\",    type=str, required=True,\n",
    "        help=\"Path to Hindi development lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--test_tsv\",   type=str, required=True,\n",
    "        help=\"Path to Hindi test lexicon TSV\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--gpu_ids\", type=int, nargs=\"+\", default=[0],\n",
    "        help=\"CUDA device IDs to use (e.g. 0 1).\"\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "def build_data_loaders(\n",
    "    train_path: str,\n",
    "    dev_path:   str,\n",
    "    test_path:  str,\n",
    "    batch_size: int,\n",
    "    use_attest: bool\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, DakshinaLexicon, DakshinaLexicon]:\n",
    "    \"\"\"\n",
    "    Builds train/dev/test datasets and loaders, re-using vocab built on train.\n",
    "    If use_attest=True, uses a WeightedRandomSampler on train counts.\n",
    "    \"\"\"\n",
    "    # build train ds + vocabs\n",
    "    train_ds = DakshinaLexicon(\n",
    "        train_path,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=use_attest\n",
    "    )\n",
    "    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n",
    "\n",
    "    # dev/test reuse same vocabs\n",
    "    dev_ds  = DakshinaLexicon(dev_path,  src_vocab, tgt_vocab)\n",
    "    test_ds = DakshinaLexicon(test_path, src_vocab, tgt_vocab)\n",
    "\n",
    "    # collate_fn for padding\n",
    "    pad_id = src_vocab.stoi[\"<pad>\"]\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=pad_id)\n",
    "\n",
    "    # train loader: optionally weighted by attestations\n",
    "    if use_attest:\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=train_ds.example_counts,\n",
    "            num_samples=len(train_ds),\n",
    "            replacement=True\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=batch_size,\n",
    "            sampler=sampler, collate_fn=collate_fn\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=batch_size,\n",
    "            shuffle=True, collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    # dev/test loaders\n",
    "    dev_loader  = DataLoader(\n",
    "        dev_ds, batch_size=batch_size,\n",
    "        shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size,\n",
    "        shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, dev_loader, test_loader, train_ds, dev_ds\n",
    "\n",
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader:   DataLoader,\n",
    "    cfg:          Seq2SeqAttentionConfig,\n",
    "    device:       torch.device,\n",
    "    epochs:       int,\n",
    "    lr:           float,\n",
    "    teacher_forcing: float\n",
    ") -> Seq2SeqAttention:\n",
    "    \"\"\"\n",
    "    Trains the Seq2SeqAttention model for `epochs` epochs.\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "    model = Seq2SeqAttention(cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn   = torch.nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader,\n",
    "            optimizer, loss_fn,\n",
    "            device, teacher_forcing\n",
    "        )\n",
    "        dev_loss   = eval_epoch(\n",
    "            model, dev_loader,\n",
    "            loss_fn, device\n",
    "        )\n",
    "        print(f\"Epoch {epoch:02d} | train ppl={math.exp(train_loss):.2f} | dev ppl={math.exp(dev_loss):.2f}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_beam_exact_match(\n",
    "    model:       Seq2SeqAttention,\n",
    "    dataset:     DakshinaLexicon,\n",
    "    beam_size:   int,\n",
    "    device:      torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Runs beam_search_decode on each example in `dataset` (batch_size=1),\n",
    "    computes exact‐match rate vs. gold target.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src_ids, tgt_ids in tqdm(dataset, desc=f\"beam={beam_size}\", leave=False):\n",
    "            total += 1\n",
    "            # prepare tensors\n",
    "            src_len = len(src_ids)\n",
    "            src_tensor = torch.tensor([src_ids], device=device)\n",
    "            len_tensor = torch.tensor([src_len], device=device)\n",
    "\n",
    "            # beam search decode\n",
    "            pred_ids = model.beam_search_decode(\n",
    "                src_tensor, len_tensor,\n",
    "                beam_size=beam_size,\n",
    "                max_len=max(src_len * 2, 50)\n",
    "            )[0].tolist()\n",
    "\n",
    "            # drop leading <sos> if present\n",
    "            if pred_ids and pred_ids[0] == dataset.tgt_vocab.stoi[\"<sos>\"]:\n",
    "                pred_ids = pred_ids[1:]\n",
    "\n",
    "            # decode strings\n",
    "            pred_str = dataset.tgt_vocab.decode(pred_ids)\n",
    "            gold_str = dataset.tgt_vocab.decode(tgt_ids[1:])\n",
    "\n",
    "            if pred_str == gold_str:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # pin GPUs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, args.gpu_ids))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # build data loaders\n",
    "    train_loader, dev_loader, _, train_ds, dev_ds = build_data_loaders(\n",
    "        train_path=args.train_tsv,\n",
    "        dev_path=args.dev_tsv,\n",
    "        test_path=args.test_tsv,    # unused here\n",
    "        batch_size=best[\"batch_size\"],\n",
    "        use_attest=best[\"use_attestations\"]\n",
    "    )\n",
    "\n",
    "    # construct model config\n",
    "    extra_cfg = {}\n",
    "    if best[\"embedding_method\"] == \"svd_ppmi\":\n",
    "        extra_cfg[\"svd_sources\"] = train_ds.encoded_sources\n",
    "\n",
    "    cfg = Seq2SeqAttentionConfig(\n",
    "        source_vocab_size=train_ds.src_vocab.size,\n",
    "        target_vocab_size=train_ds.tgt_vocab.size,\n",
    "        embedding_dim=best[\"embedding_size\"],\n",
    "        hidden_dim=best[\"hidden_size\"],\n",
    "        encoder_layers=best[\"encoder_layers\"],\n",
    "        decoder_layers=best[\"decoder_layers\"],\n",
    "        cell_type=best[\"cell_type\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        pad_index=train_ds.src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=train_ds.tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=train_ds.tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=best[\"embedding_method\"],\n",
    "        **extra_cfg,\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    model = train_model(\n",
    "        train_loader, dev_loader,\n",
    "        cfg=cfg,\n",
    "        device=device,\n",
    "        epochs=best[\"epochs\"],\n",
    "        lr=best[\"learning_rate\"],\n",
    "        teacher_forcing=best[\"teacher_forcing\"]\n",
    "    )\n",
    "\n",
    "    # tune beam size on dev set\n",
    "    print(\"\\nTuning beam size on dev set (exact‐match rate):\")\n",
    "    for beam in [1, 2, 3, 5, 8, 10]:\n",
    "        acc = evaluate_beam_exact_match(\n",
    "            model, dev_ds,\n",
    "            beam_size=beam,\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"  beam_size={beam:2d} → dev accuracy = {acc * 100:5.2f}%\")\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simulate exactly the CLI invocation:\n",
    "sys.argv = [\n",
    "    \"solution_5a.py\",\n",
    "    \"--train_tsv\", \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\",   \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\",  \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--gpu_ids\",   \"3\",\n",
    "]\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce026d",
   "metadata": {},
   "source": [
    "#### Q5 Solution - Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q5.b: Train (with early stopping) & evaluate best attention‐augmented Seq2Seq model on the Dakshina Hindi test set.\n",
    "\n",
    "This script:\n",
    "  1. Loads train/dev/test lexicons and builds/uses the same vocabularies.\n",
    "  2. Constructs the Seq2SeqAttention model with your best hyperparameters.\n",
    "  3. If no checkpoint exists:\n",
    "       - Trains with early stopping on dev cross‐entropy loss (patience=3).\n",
    "       - Saves the best‐so‐far model to --checkpoint.\n",
    "     Otherwise loads the existing checkpoint.\n",
    "  4. Runs beam-search decoding on the test set, computes exact‐match accuracy.\n",
    "  5. Saves all (source, gold, prediction) triples under `predictions_attention/` as TSV and CSV.\n",
    "  6. Samples 20 predictions, builds a colored table figure, saves it, and logs it to W&B.\n",
    "  7. Selects 10 random test examples, computes their greedy attention heatmaps, and\n",
    "     plots them in a 3×4 grid, saving & logging the figure to W&B.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "    python solution_5b.py \\\n",
    "      --train_tsv ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "      --dev_tsv   ./lexicons/hi.translit.sampled.dev.tsv \\\n",
    "      --test_tsv  ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "      --checkpoint ./checkpoints/best_attention.pt \\\n",
    "      --output_dir predictions_attention \\\n",
    "      --gpu_ids 3 \\\n",
    "      --wandb_project DA6401_Intro_to_DeepLearning_Assignment_3 \\\n",
    "      --wandb_run_name solution_5b_run \\\n",
    "      --wandb_run_tag solution_5b\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ────────────────────────────────\n",
    "# Download & register Devanagari font\n",
    "# ────────────────────────────────\n",
    "font_dir = Path.cwd() / \"fonts\"\n",
    "font_dir.mkdir(exist_ok=True)\n",
    "font_path = font_dir / \"Hind-Regular.ttf\"\n",
    "\n",
    "if not font_path.exists():\n",
    "    print(\"Font file not found. Please ensure 'Hind-Regular.ttf' is in the 'fonts' directory which\"\n",
    "    \"can be downloaded from Google Fonts. https://www.cufonfonts.com/font/noto-sans-devanagari\")\n",
    "else:\n",
    "    fm.fontManager.addfont(str(font_path))\n",
    "    plt.rcParams[\"font.family\"] = \"Hind\"\n",
    "    plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "    print(\"Font loaded and matplotlib configured.\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# Replace these with best attention‐model hyperparameters:\n",
    "best = {\n",
    "    \"batch_size\":        128,\n",
    "    \"beam_size\":         3,\n",
    "    \"cell_type\":       \"GRU\",\n",
    "    \"decoder_layers\":     2,\n",
    "    \"dropout\":          0.2,\n",
    "    \"embedding_method\":\"svd_ppmi\",\n",
    "    \"embedding_size\":   64,\n",
    "    \"encoder_layers\":     1,\n",
    "    \"hidden_size\":      512,\n",
    "    \"learning_rate\":   0.0006899910999897612,\n",
    "    \"teacher_forcing\":  0.5,\n",
    "    \"use_attestations\": True,\n",
    "    # early stopping patience\n",
    "    \"patience\":          3,\n",
    "    # number of training epochs to try\n",
    "    \"epochs\":           25,\n",
    "}\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Q5.b: Train (with early stopping) & evaluate best attention model\"\n",
    "    )\n",
    "    p.add_argument(\"--train_tsv\",     type=str, required=True, help=\"Path to train lexicon TSV\")\n",
    "    p.add_argument(\"--dev_tsv\",       type=str, required=True, help=\"Path to dev lexicon TSV\")\n",
    "    p.add_argument(\"--test_tsv\",      type=str, required=True, help=\"Path to test lexicon TSV\")\n",
    "    p.add_argument(\"--checkpoint\",    type=str, required=True, help=\"Path to save/load model checkpoint\")\n",
    "    p.add_argument(\"--output_dir\",    type=str, default=\"predictions_attention\",\n",
    "                   help=\"Directory to write predictions.tsv/csv\")\n",
    "    p.add_argument(\"--gpu_ids\",       type=int, nargs=\"+\", default=[0], help=\"CUDA device IDs\")\n",
    "    p.add_argument(\"--wandb_project\", type=str, default=None, help=\"W&B project name\")\n",
    "    p.add_argument(\"--wandb_run_name\",type=str, default=None, help=\"W&B run name\")\n",
    "    p.add_argument(\"--wandb_run_tag\", type=str, default=\"solution_5b\", help=\"W&B run tag\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def get_attention_heatmap(\n",
    "    model: Seq2SeqAttention,\n",
    "    src_ids: list[int],\n",
    "    src_lens: torch.Tensor,\n",
    "    src_vocab: DakshinaLexicon.src_vocab.__class__,\n",
    "    tgt_vocab: DakshinaLexicon.tgt_vocab.__class__,\n",
    "    device: torch.device,\n",
    "    max_len: int = 50\n",
    ") -> tuple[list[int], list[list[float]]]:\n",
    "    \"\"\"\n",
    "    Run greedy decode step-by-step, collecting the attention weights at each step.\n",
    "    Returns (predicted_ids, attention_weights_matrix) where matrix[t][i] is\n",
    "    the attention weight at decoder time t for encoder position i.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # prepare tensors\n",
    "    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "    enc_outputs, enc_hidden = model.encoder(src_tensor, src_lens)\n",
    "    enc_mask = (src_tensor != model.cfg.pad_index).to(device)\n",
    "    dec_hidden = _align_hidden_state(enc_hidden, model.cfg.decoder_layers)\n",
    "    # start with <sos>\n",
    "    dec_input = torch.full((1,1), model.cfg.sos_index, dtype=torch.long, device=device)\n",
    "\n",
    "    predicted_ids: list[int] = []\n",
    "    attention_weights: list[list[float]] = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, dec_hidden, align = model.decoder(\n",
    "            dec_input, dec_hidden, enc_outputs, enc_mask\n",
    "        )\n",
    "        # align: (1, T_src) → record\n",
    "        alignment = align.squeeze(0).tolist()\n",
    "        attention_weights.append(alignment)\n",
    "        # pick argmax\n",
    "        dec_input = logits.argmax(-1)  # (1,1)\n",
    "        next_id = dec_input.item()\n",
    "        if next_id == model.cfg.eos_index:\n",
    "            break\n",
    "        predicted_ids.append(next_id)\n",
    "\n",
    "    return predicted_ids, attention_weights\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # ─── Pin GPUs & select device ────────────────────────────────────\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, args.gpu_ids))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ─── Init WandB if requested ─────────────────────────────────────\n",
    "    use_wandb = args.wandb_project is not None\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            name=args.wandb_run_name,\n",
    "            tags=[args.wandb_run_tag],\n",
    "            config=best\n",
    "        )\n",
    "\n",
    "    # ─── Ensure output directories exist ──────────────────────────────\n",
    "    output_path = Path(args.output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = Path(args.checkpoint)\n",
    "    ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ─── Build vocab from train set ──────────────────────────────────\n",
    "    train_ds = DakshinaLexicon(\n",
    "        args.train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=best[\"use_attestations\"]\n",
    "    )\n",
    "    src_vocab = train_ds.src_vocab\n",
    "    tgt_vocab = train_ds.tgt_vocab\n",
    "\n",
    "    # ─── Prepare dev/test datasets ───────────────────────────────────\n",
    "    collate_fn = lambda batch: collate_batch(batch, pad_id=src_vocab.stoi[\"<pad>\"])\n",
    "    train_loader = DataLoader(train_ds, batch_size=best[\"batch_size\"],\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "    dev_ds = DakshinaLexicon(args.dev_tsv, src_vocab, tgt_vocab)\n",
    "    dev_loader = DataLoader(dev_ds, batch_size=best[\"batch_size\"],\n",
    "                            shuffle=False, collate_fn=collate_fn)\n",
    "    test_ds = DakshinaLexicon(args.test_tsv, src_vocab, tgt_vocab)\n",
    "    test_loader = DataLoader(test_ds, batch_size=best[\"batch_size\"],\n",
    "                             shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # ─── Build the Seq2SeqAttention model ────────────────────────────\n",
    "    extra = {}\n",
    "    if best[\"embedding_method\"] == \"svd_ppmi\":\n",
    "        extra[\"svd_sources\"] = train_ds.encoded_sources\n",
    "\n",
    "    cfg = Seq2SeqAttentionConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=best[\"embedding_size\"],\n",
    "        hidden_dim=best[\"hidden_size\"],\n",
    "        encoder_layers=best[\"encoder_layers\"],\n",
    "        decoder_layers=best[\"decoder_layers\"],\n",
    "        cell_type=best[\"cell_type\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=best[\"embedding_method\"],\n",
    "        **extra\n",
    "    )\n",
    "    model = Seq2SeqAttention(cfg).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best[\"learning_rate\"])\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=cfg.pad_index)\n",
    "\n",
    "    # ─── Train with early stopping if checkpoint missing ─────────────\n",
    "    if not ckpt_path.exists():\n",
    "        print(\"No checkpoint found; starting training with early stopping...\")\n",
    "        best_dev_loss = float(\"inf\")\n",
    "        no_improve = 0\n",
    "\n",
    "        for epoch in range(1, best[\"epochs\"] + 1):\n",
    "            train_loss = train_epoch(\n",
    "                model, train_loader, optimizer, loss_fn,\n",
    "                device, teacher_forcing=best[\"teacher_forcing\"]\n",
    "            )\n",
    "            dev_loss = eval_epoch(model, dev_loader, loss_fn, device)\n",
    "            train_ppl = math.exp(train_loss)\n",
    "            dev_ppl   = math.exp(dev_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch:02d} | \"\n",
    "                  f\"train_loss={train_loss:.4f} ppl={train_ppl:.2f} | \"\n",
    "                  f\"dev_loss={dev_loss:.4f} ppl={dev_ppl:.2f}\")\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    \"Q5b_epoch\":        epoch,\n",
    "                    \"Q5b_train_loss\":   train_loss,\n",
    "                    \"Q5b_train_ppl\":    train_ppl,\n",
    "                    \"Q5b_dev_loss\":     dev_loss,\n",
    "                    \"Q5b_dev_ppl\":      dev_ppl,\n",
    "                })\n",
    "\n",
    "            # early stopping on dev_loss\n",
    "            if dev_loss < best_dev_loss:\n",
    "                best_dev_loss = dev_loss\n",
    "                no_improve = 0\n",
    "                torch.save({\"model_state_dict\": model.state_dict()}, str(ckpt_path))\n",
    "                print(\"  ↳ dev improved; checkpoint saved.\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                print(f\"  ↳ no improvement for {no_improve} epoch(s)\")\n",
    "                if no_improve >= best[\"patience\"]:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "        print(\"Training complete.\\n\")\n",
    "    else:\n",
    "        print(f\"Found existing checkpoint at {ckpt_path}; skipping training.\\n\")\n",
    "\n",
    "    # ─── Load the best checkpoint ─────────────────────────────────────\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # ─── Decode test set & compute exact‐match accuracy ──────────────\n",
    "    total, correct = 0, 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src_batch, src_lens, tgt_batch in test_loader:\n",
    "            for i in range(src_batch.size(0)):\n",
    "                total += 1\n",
    "                # Trim to true length to fix mask/score mismatch\n",
    "                length = src_lens[i].item()\n",
    "                s = src_batch[i, :length].unsqueeze(0).to(device)\n",
    "                l = torch.tensor([length], device=device)\n",
    "\n",
    "                pred_ids = model.beam_search_decode(\n",
    "                    s, l,\n",
    "                    beam_size=best[\"beam_size\"],\n",
    "                    max_len=50\n",
    "                )[0].tolist()\n",
    "\n",
    "                # drop leading <sos> if present\n",
    "                if pred_ids and pred_ids[0] == tgt_vocab.stoi[\"<sos>\"]:\n",
    "                    pred_ids = pred_ids[1:]\n",
    "\n",
    "                src_str  = src_vocab.decode(s[0].tolist())\n",
    "                gold_str = tgt_vocab.decode(tgt_batch[i].tolist()[1:])\n",
    "                pred_str = tgt_vocab.decode(pred_ids)\n",
    "\n",
    "                if pred_str == gold_str:\n",
    "                    correct += 1\n",
    "                predictions.append((src_str, gold_str, pred_str))\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"\\nTest exact‐match accuracy: {accuracy:.2f}% ({correct}/{total})\\n\")\n",
    "    if use_wandb:\n",
    "        wandb.log({\"Q5b_test_accuracy\": accuracy})\n",
    "\n",
    "    # ─── Save all predictions ─────────────────────────────────────────\n",
    "    df = pd.DataFrame(predictions, columns=[\"source\", \"target\", \"prediction\"])\n",
    "    df.to_csv(output_path / \"predictions.tsv\", sep=\"\\t\", index=False)\n",
    "    df.to_csv(output_path / \"predictions.csv\", index=False)\n",
    "    print(f\"Saved predictions → {output_path/'predictions.tsv'}, {output_path/'predictions.csv'}\\n\")\n",
    "\n",
    "    # ─── Sample 20 and build colored table figure ─────────────────────\n",
    "    sample_df = df.sample(20, random_state=42)\n",
    "    # green for correct, red for wrong\n",
    "    colors = [\n",
    "        [\"#c8e6c9\" if row.target == row.prediction else \"#ffcdd2\"\n",
    "         for _ in sample_df.columns]\n",
    "        for _, row in sample_df.iterrows()\n",
    "    ]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.axis(\"off\")\n",
    "    tbl = ax.table(\n",
    "        cellText=sample_df.values.tolist(),\n",
    "        colLabels=sample_df.columns.tolist(),\n",
    "        cellColours=colors,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\"\n",
    "    )\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(10)\n",
    "    tbl.scale(1, 2)\n",
    "    figure_path = output_path / \"sample_predictions.png\"\n",
    "    fig.savefig(figure_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved sample predictions figure to {figure_path}\")\n",
    "    if use_wandb:\n",
    "        wandb.log({\"Q5b_sample_table\": wandb.Image(str(figure_path))})\n",
    "    \n",
    "    # ─── Attention heatmaps for 10 random test examples ──────────────\n",
    "    random.seed(42)\n",
    "    indices = random.sample(range(len(test_ds)), 10)\n",
    "    heatmaps = []\n",
    "    src_lists, pred_lists = [], []\n",
    "    for idx in indices:\n",
    "        src_ids, tgt_ids = test_ds[idx]\n",
    "        _, attn = get_attention_heatmap(\n",
    "            model, src_ids, torch.tensor([len(src_ids)], device=device),\n",
    "            src_vocab, tgt_vocab, device\n",
    "        )\n",
    "        heatmaps.append(attn)\n",
    "        src_lists.append(src_ids)\n",
    "        # we drop <sos> from predictions\n",
    "        pred_ids = [i for i in attn and []]  # placeholder\n",
    "\n",
    "    # actually regenerate preds & char lists\n",
    "    preds_and_attn = []\n",
    "    for src_ids in src_lists:\n",
    "        pred_ids, attn = get_attention_heatmap(\n",
    "            model, src_ids, torch.tensor([len(src_ids)], device=device),\n",
    "            src_vocab, tgt_vocab, device\n",
    "        )\n",
    "        # drop any leading <sos>\n",
    "        if pred_ids and pred_ids[0] == tgt_vocab.stoi[\"<sos>\"]:\n",
    "            pred_ids = pred_ids[1:]\n",
    "            attn = attn[1:]\n",
    "        preds_and_attn.append((src_ids, pred_ids, attn))\n",
    "\n",
    "    # plot in a 3×4 grid\n",
    "    n = len(preds_and_attn)\n",
    "    cols = 3\n",
    "    rows = math.ceil(n/cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))\n",
    "    axes = axes.flatten()\n",
    "    for i, (src_ids, pred_ids, attn) in enumerate(preds_and_attn):\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(attn, aspect=\"auto\", origin=\"lower\")\n",
    "        # x-axis: source chars\n",
    "        sx = [src_vocab.itos[id] for id in src_ids]\n",
    "        ax.set_xticks(range(len(sx)))\n",
    "        ax.set_xticklabels(sx, rotation=90, fontsize=8)\n",
    "        # y-axis: predicted chars\n",
    "        py = [tgt_vocab.itos[id] for id in pred_ids]\n",
    "        ax.set_yticks(range(len(py)))\n",
    "        ax.set_yticklabels(py, fontsize=8)\n",
    "        ax.set_xlabel(\"Source\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        ax.set_title(f\"Example {i+1}\")\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    # turn off any extra axes\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    heatmap_path = output_path/\"attention_heatmaps.png\"\n",
    "    fig.savefig(heatmap_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved attention heatmaps to {heatmap_path}\")\n",
    "    if use_wandb:\n",
    "        wandb.log({\"Q5b_attention_heatmaps\": wandb.Image(str(heatmap_path))})\n",
    "        wandb.finish()\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simulate the exact CLI invocation:\n",
    "sys.argv = [\n",
    "    \"solution_5b.py\",\n",
    "    \"--train_tsv\",    \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--dev_tsv\",      \"../lexicons/hi.translit.sampled.dev.tsv\",\n",
    "    \"--test_tsv\",     \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--checkpoint\",   \"../checkpoints/best_attention_.pt\",\n",
    "    \"--output_dir\",   \"../predictions_attention_\",\n",
    "    \"--gpu_ids\",      \"3\",\n",
    "    \"--wandb_project\",\"transliteration\",\n",
    "    \"--wandb_run_name\",\"solution_5b_run\",\n",
    "    \"--wandb_run_tag\",\"solution_5b\",\n",
    "]\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be822680",
   "metadata": {},
   "source": [
    "# Q6 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49737c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q6: Interactive “Connectivity” visualization for your attention-augmented Seq2Seq model\n",
    "\n",
    "This script lets you:\n",
    "  1. Load your trained Seq2SeqAttention checkpoint.\n",
    "  2. Sample N examples from the test set.\n",
    "  3. For each example, run a greedy decode while recording the attention weights\n",
    "     at each decoder timestep.\n",
    "  4. Save a standalone HTML that displays, for each example:\n",
    "       - The source characters along the x-axis.\n",
    "       - The predicted output characters along the y-axis.\n",
    "       - A Plotly heatmap of attention weights.\n",
    "     You can hover over any cell to see the exact weight—and thus see “connectivity.”\n",
    "  5. (New!) Log the final HTML to WandB so you can browse it in your project.\n",
    "\n",
    "Usage (as a script):\n",
    "\n",
    "    python solution_6.py \\\n",
    "      --checkpoint ./checkpoints/best_attention.pt \\\n",
    "      --train_tsv   ./lexicons/hi.translit.sampled.train.tsv \\\n",
    "      --test_tsv    ./lexicons/hi.translit.sampled.test.tsv \\\n",
    "      --n_examples  5 \\\n",
    "      --output_html connectivity.html \\\n",
    "      --wandb_project DA6401_Intro_to_DeepLearning_Assignment_3 \\\n",
    "      --wandb_run_name solution_6_run \\\n",
    "      --wandb_run_tag  solution_6\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import wandb\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Hyper-parameters for your best attention model (must match how you trained it)\n",
    "best = {\n",
    "    \"batch_size\":        128,\n",
    "    \"beam_size\":         3,\n",
    "    \"cell_type\":         \"GRU\",\n",
    "    \"decoder_layers\":    3,\n",
    "    \"dropout\":           0.2,\n",
    "    \"embedding_method\":  \"svd_ppmi\",\n",
    "    \"embedding_size\":    64,\n",
    "    \"encoder_layers\":    1,\n",
    "    \"hidden_size\":       512,\n",
    "    \"learning_rate\":     0.0006899910999897612,\n",
    "    \"teacher_forcing\":   0.5,\n",
    "    \"use_attestations\":  True,\n",
    "    # early stopping patience\n",
    "    \"patience\":         3,\n",
    "    # number of training epochs to try\n",
    "    \"epochs\":          20,\n",
    "}\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Q6: Visualize character-level attention connectivity interactively\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--checkpoint\", type=str, required=True,\n",
    "        help=\"Path to your trained Seq2SeqAttention .pt file\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--train_tsv\", type=str, required=True,\n",
    "        help=\"TSV of the TRAIN split (native\\tr omanized\\tcount) for building vocab\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--test_tsv\", type=str, required=True,\n",
    "        help=\"TSV of the TEST split (native\\tr omanized\\tcount)\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--n_examples\", type=int, default=3,\n",
    "        help=\"Number of random examples to visualize\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--output_html\", type=str, default=\"connectivity.html\",\n",
    "        help=\"Path to save the standalone HTML with Plotly plots\"\n",
    "    )\n",
    "    # WandB logging args\n",
    "    p.add_argument(\n",
    "        \"--wandb_project\", type=str, default=None,\n",
    "        help=\"(optional) WandB project name to log this HTML\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--wandb_run_name\", type=str, default=None,\n",
    "        help=\"(optional) WandB run name\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--wandb_run_tag\", type=str, default=None,\n",
    "        help=\"(optional) WandB run tag\"\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "def load_model_and_vocab(\n",
    "    ckpt_path: str,\n",
    "    train_tsv: str,\n",
    "    device: torch.device\n",
    ") -> tuple[Seq2SeqAttention, DakshinaLexicon, DakshinaLexicon]:\n",
    "    \"\"\"\n",
    "    Loads the checkpoint into a Seq2SeqAttention model,\n",
    "    and builds the vocab from the TRAIN split so that src/tgt vocabs align.\n",
    "    \"\"\"\n",
    "    # 1) Build the vocabulary from the true training split\n",
    "    train_ds = DakshinaLexicon(\n",
    "        train_tsv,\n",
    "        build_vocabs=True,\n",
    "        use_attestations=best[\"use_attestations\"]\n",
    "    )\n",
    "    src_vocab = train_ds.src_vocab\n",
    "    tgt_vocab = train_ds.tgt_vocab\n",
    "\n",
    "    # 2) Peek at the checkpoint to detect how many decoder layers it actually has\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state_dict = ckpt[\"model_state_dict\"]\n",
    "\n",
    "    # Collect all weight keys of the form \"decoder.rnn.weight_ih_l{n}\"\n",
    "    layer_indices = []\n",
    "    for key in state_dict:\n",
    "        if key.startswith(\"decoder.rnn.weight_ih_l\"):\n",
    "            # e.g. \"decoder.rnn.weight_ih_l0\", \"decoder.rnn.weight_ih_l1\", ...\n",
    "            idx_str = key.split(\"decoder.rnn.weight_ih_l\", 1)[1]\n",
    "            try:\n",
    "                layer_indices.append(int(idx_str))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    if layer_indices:\n",
    "        # max index + 1 = number of layers\n",
    "        actual_decoder_layers = max(layer_indices) + 1\n",
    "        print(f\"  ↳ Checkpoint has {actual_decoder_layers} decoder layers (detected)\")\n",
    "    else:\n",
    "        actual_decoder_layers = best[\"decoder_layers\"]\n",
    "        print(f\"  ↳ No decoder.rnn.weight_ih_l* keys found; defaulting to {actual_decoder_layers}\")\n",
    "\n",
    "    # 3) Construct exactly the same config you used for training,\n",
    "    #    except use the detected number of decoder layers:\n",
    "    cfg = Seq2SeqAttentionConfig(\n",
    "        source_vocab_size=src_vocab.size,\n",
    "        target_vocab_size=tgt_vocab.size,\n",
    "        embedding_dim=best[\"embedding_size\"],\n",
    "        hidden_dim=best[\"hidden_size\"],\n",
    "        encoder_layers=best[\"encoder_layers\"],\n",
    "        decoder_layers=actual_decoder_layers,\n",
    "        cell_type=best[\"cell_type\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        pad_index=src_vocab.stoi[\"<pad>\"],\n",
    "        sos_index=tgt_vocab.stoi[\"<sos>\"],\n",
    "        eos_index=tgt_vocab.stoi[\"<eos>\"],\n",
    "        embedding_method=best[\"embedding_method\"],\n",
    "        **({\"svd_sources\": train_ds.encoded_sources}\n",
    "           if best[\"embedding_method\"] == \"svd_ppmi\" else {})\n",
    "    )\n",
    "\n",
    "    # 4) Instantiate and load\n",
    "    model = Seq2SeqAttention(cfg).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    return model, src_vocab, tgt_vocab\n",
    "\n",
    "def record_attention(\n",
    "    model: Seq2SeqAttention,\n",
    "    src_ids: list[int],\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device: torch.device,\n",
    "    max_len: int = 50\n",
    ") -> tuple[list[str], list[str], list[list[float]]]:\n",
    "    \"\"\"\n",
    "    Greedy-decode `src_ids` with the model, capturing attention weights at each step.\n",
    "    Returns:\n",
    "      (source_chars, predicted_chars, attention_matrix)\n",
    "    where\n",
    "      attention_matrix[t][i] = score attending to src position i when predicting\n",
    "                              output char at position t\n",
    "    \"\"\"\n",
    "    # Prepare tensors\n",
    "    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "    src_len    = torch.tensor([len(src_ids)], device=device)\n",
    "\n",
    "    # Encoder: get outputs and initial hidden\n",
    "    enc_outputs, enc_hidden = model.encoder(src_tensor, src_len)\n",
    "    enc_mask = (src_tensor != model.cfg.pad_index).to(device)\n",
    "    dec_hidden = _align_hidden_state(enc_hidden, model.cfg.decoder_layers)\n",
    "\n",
    "    # Initialize decoder input (<sos>)\n",
    "    dec_input = torch.full((1,1), model.cfg.sos_index, dtype=torch.long, device=device)\n",
    "    predicted_ids: list[int] = []\n",
    "    attentions: list[list[float]] = []\n",
    "\n",
    "    # Step through decoder, capture alignments\n",
    "    for _ in range(max_len):\n",
    "        logits, dec_hidden, alignments = model.decoder(\n",
    "            dec_input, dec_hidden, enc_outputs, enc_mask\n",
    "        )\n",
    "        # record attention over source\n",
    "        attn = alignments.squeeze(0).tolist()\n",
    "        attentions.append(attn)\n",
    "\n",
    "        # next token\n",
    "        dec_input = logits.argmax(-1)  # shape (1,1)\n",
    "        next_id = dec_input.item()\n",
    "        if next_id == model.cfg.eos_index:\n",
    "            break\n",
    "        predicted_ids.append(next_id)\n",
    "\n",
    "    # Convert indices back to characters\n",
    "    source_chars    = [src_vocab.itos[i] for i in src_ids]\n",
    "    predicted_chars = [tgt_vocab.itos[i] for i in predicted_ids]\n",
    "    return source_chars, predicted_chars, attentions\n",
    "\n",
    "def make_plotly_figure(\n",
    "    source_chars:    list[str],\n",
    "    predicted_chars: list[str],\n",
    "    attentions:      list[list[float]],\n",
    "    title:           str\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Builds a Plotly heatmap figure with x=source_chars, y=predicted_chars,\n",
    "    z=attentions matrix.  Hover text will show exact weight.\n",
    "    \"\"\"\n",
    "    heatmap = go.Heatmap(\n",
    "        z=attentions,\n",
    "        x=source_chars,\n",
    "        y=predicted_chars,\n",
    "        colorscale=\"Blues\",\n",
    "        zmin=0, zmax=1,\n",
    "        colorbar=dict(\n",
    "            title=dict(\n",
    "                text=\"Attention\",\n",
    "                side=\"right\",\n",
    "                font=dict(size=12)\n",
    "            ),\n",
    "            lenmode=\"fraction\",\n",
    "            len=0.6,\n",
    "            tickfont=dict(size=10)\n",
    "        ),\n",
    "        hovertemplate=(\n",
    "            \"input: %{x}<br>\"\n",
    "            \"output: %{y}<br>\"\n",
    "            \"weight: %{z:.3f}\"\n",
    "            \"<extra></extra>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[heatmap])\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            x=0.5,           # center\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"top\",\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        margin=dict(l=80, r=50, t=100, b=80),\n",
    "        width=600,\n",
    "        height=450,\n",
    "        xaxis=dict(\n",
    "            title=dict(\n",
    "                text=\"Source characters\",\n",
    "                font=dict(size=14)\n",
    "            ),\n",
    "            tickangle=-45,\n",
    "            tickfont=dict(size=12),\n",
    "            side=\"top\",\n",
    "            automargin=True\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(\n",
    "                text=\"Predicted characters\",\n",
    "                font=dict(size=14)\n",
    "            ),\n",
    "            tickfont=dict(size=12),\n",
    "            automargin=True\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # ─── WandB initialization (optional) ─────────────────────────\n",
    "    use_wandb = args.wandb_project is not None\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            name=args.wandb_run_name,\n",
    "            tags=[args.wandb_run_tag] if args.wandb_run_tag else None\n",
    "        )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load model and vocab built from the true training split\n",
    "    model, src_vocab, tgt_vocab = load_model_and_vocab(\n",
    "        args.checkpoint, args.train_tsv, device\n",
    "    )\n",
    "\n",
    "    # 2) Load test set (for sampling)\n",
    "    test_ds = DakshinaLexicon(args.test_tsv, src_vocab, tgt_vocab)\n",
    "\n",
    "    # 3) Randomly pick N examples\n",
    "    random.seed(42)\n",
    "    selected_indices = random.sample(range(len(test_ds)), k=args.n_examples)\n",
    "\n",
    "    # 4) For each example, record attention and make a Plotly figure\n",
    "    figures: list[go.Figure] = []\n",
    "    for idx_rank, idx in enumerate(selected_indices, start=1):\n",
    "        src_ids, _ = test_ds[idx]\n",
    "        source_chars, predicted_chars, attn_matrix = record_attention(\n",
    "            model, src_ids, src_vocab, tgt_vocab, device\n",
    "        )\n",
    "        title = (\n",
    "            f'Example {idx_rank}: “{\"\".join(source_chars)}” → '\n",
    "            f'“{\"\".join(predicted_chars)}”'\n",
    "        )\n",
    "        fig = make_plotly_figure(source_chars, predicted_chars, attn_matrix, title)\n",
    "        figures.append(fig)\n",
    "\n",
    "    # 5) Assemble all figures into one standalone HTML\n",
    "    html_snippets = [\n",
    "        fig.to_html(full_html=False, include_plotlyjs=\"cdn\")\n",
    "        for fig in figures\n",
    "    ]\n",
    "    html_body = \"\\n<hr>\\n\".join(html_snippets)\n",
    "    full_html = f\"\"\"\\\n",
    "<html>\n",
    "  <head><meta charset=\"utf-8\"/><title>Attention Connectivity</title></head>\n",
    "  <body>\n",
    "    <h1>Q6: Attention Connectivity Visualizations</h1>\n",
    "    {html_body}\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    # 6) Write out the HTML\n",
    "    out_path = Path(args.output_html)\n",
    "    out_path.write_text(full_html, encoding=\"utf-8\")\n",
    "    print(f\"Wrote interactive connectivity HTML to {out_path}\")\n",
    "\n",
    "    # 7) Log to WandB (if requested)\n",
    "    if use_wandb:\n",
    "        # wandb.Html will render your HTML in the WandB UI\n",
    "        wandb.log({\n",
    "            \"connectivity\": wandb.Html(str(out_path))\n",
    "        })\n",
    "        wandb.finish()\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simulate exactly the CLI invocation:\n",
    "sys.argv = [\n",
    "    \"solution_6.py\",\n",
    "    \"--checkpoint\",    \"../checkpoints/best_attention.pt\",\n",
    "    \"--train_tsv\",     \"../lexicons/hi.translit.sampled.train.tsv\",\n",
    "    \"--test_tsv\",      \"../lexicons/hi.translit.sampled.test.tsv\",\n",
    "    \"--n_examples\",    \"5\",\n",
    "    \"--output_html\",   \"../connectivity.html\",\n",
    "    \"--wandb_project\", \"transliteration\",\n",
    "    \"--wandb_run_name\",\"solution_6_run\",\n",
    "    \"--wandb_run_tag\", \"solution_6\",\n",
    "]\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc3fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_jax_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
